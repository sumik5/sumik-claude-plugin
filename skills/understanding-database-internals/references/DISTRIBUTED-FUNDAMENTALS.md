# 分散システム基礎

第8章「基本事項の紹介と概要」のリファレンス

## 並行実行

複数のスレッドが同じ変数に同時アクセスすると、スケジュールによって異なる結果が生じる。

### 並行実行の例

```
int i = 1;
i += 2;
i *= 2;
```

実行順序によって結果が変わる：

- **x = 2**: 両方のスレッドが初期値を読み取り、加算が計算後の値を書き出すが、その値が乗算の結果で上書きされる場合
- **x = 3**: 両方のスレッドが初期値を読み取り、乗算が計算後の値を書き出すが、その値が加算の結果で上書きされる場合
- **x = 4**: 乗算が初期値を読み取り、その演算を実行した後で、加算が開始される場合
- **x = 6**: 加算が初期値を読み取り、その演算を実行した後で、乗算が開始される場合

### 並行と並行の違い

**並行（concurrent）とコンピューティング（並列処理、parallel）の違い：**

- **並行**: ステップのシーケンスが2通りあって、それらが並行に実行される場合、両方とも処理中になりますが、任意の時点で実行されているのは、その一方だけです。2通りのシーケンスが並行に実行される場合、それらのステップは同時に実行されます
- **並列実行**: 複数のプロセッサで実行されます

Erlangというプログラミング言語の作成者であるJoe Armstrongは、次のような例を示しました：

- 並行実行は、1台のコーヒーメーカーに対して2つのキューがあるようなものです
- 並列実行は、2台のコーヒーメーカー（2つのキューがあります）になります

本書で扱うアルゴリズムの中には、並行実行において複数の並行実行スレッドを持つシステムの説明で並行（concurrency）という用語を使用しており、並行（parallelism）という用語はほとんど使用されていないものです。

### 分散システムでの共有された状態

共有メモリの概念、たとえば単一の情報源としてのデータベースなど、分散システムに導入することができます。そのシステムは複数のノードから構成されていることは依然できません。

データベースにアクセスするには、状態を問い合わせたり変更したりするメッセージを送受信することで、通信機構を経由する必要があります。しかし、プロセスの1つが、データベースから現在の状態を問い合わせたあと、その質問に答えるには、まず競合した変更が起きないかどうかを確認する必要があります。そのためには、システムを同期性の面から説明しなければなりません。つまり、通信が完全に非同期であるかどうか、またはタイミングの前提が存在するかどうかを考慮しなければなりません。これらのタイミングの前提は、操作のタイムアウトおよび前述の存在に関係しています。

データベースが応答していない理由は、負荷がかかりすぎているからか、利用不可であるからか、または障害が近くなっているからのか、あるいは遅切のネットワークに問題があるためなのかは判然としません。これは、クラッシュの性質を説明しています。プロセスは、次のアルゴリズムのステップに参加して失敗したり、一部のメッセージが除外されたりすることと原因でクラッシュするかもしれません。そこで、障害モデルを記述して、障害の対処法は決定する際に、それらが発生しうる方法を記述することが必要になります。

システムの信頼性、つまり障害が存在していても正しく動作を続けられるかを表す特性を、フォールトトレランスと呼びます。したがって、信頼性の高いコンポーネントでシステムを構築する必要があり、面述の、単一ノードデータベースの形式での単一障害点を排除することは、この方針における重要なステップになります。それは、冗長性の導入とバックアップデータベースの追加です。しかし、ここで別の問題に突き当たります。それは、共有された状態を持つ複数のコピーをどのように同期させるかということです。

これまでのところ、単純なシステムに共有された状態を導入しようとすると、解決すべきより多くの問題が残ります。今では、状態の共有がデータベースを導入するだけといった単純なものではないことが明らかしたので、さらに詳細なアプローチを取り入れて、独立したプロセスと共有状態のないアプローチから、相互作用のメッセージのやり取りという観点から、相互作用が起こらなければなりません。

## 分散コンピューティングの誤謬

理想的なケースでは、2台のコンピュータがネットワークを介して通信するときに、すべてが適正に動作します。プロセスによって接続が開かれ、データが送信され、応答を受け取って、全てが正しく機能するということができるといった前提を下敷にしますが、何が間違が発生し、その仮定が成り立たなったときが判明した場合、システムは予測が困難な、または不可能な動作を引き起こすかもです。

大半の場合、ネットワークは信頼できるという仮定は、妥当なものです。少なくともある程度まで限界があると仮定していきつづけなければならないときは、接続するのには立地条件が必要です。リンクが何かの原因で失われたり、レイテンシについての仮定は有力かなくすぎさり、中でもレイテンシはゼロであるという仮定は、決して持つべきではありません。メッセージがリモートに正常に到達するまでには、リソースを使い尽くす可能性があります。メッセージが発行されたネットワーク上の経路は可変的がありますし、応答が失われた可能性があります。そうなければ、これらの操作は、すべて瞬時には行われません。

Michael Lewisは、「フラッシュ・ボーイズ：10億分の1秒の男たち」（文芸春秋、原書"Flash Boys"）において、ラウンドトリップの距離を改善するために近接するように舎わぜてなりつある方法について取られた手段のような、レイテンシを破壊し短縮した企業の話をしています。これは、競争上の優位性としてレイテンシを使用した話例ですが、BARTLETTIなどの他の研究によると、スモールクオートの欺瞞的（競争者用より早く価格を知って注文を実行できる能力を用いて注文を実行できる能力）の厳格な規制に対する懸念を作すことができる場合ではないということなので、その値は許容可能な値があります。

これまでの教訓として、両読行や再接続を追加し、瞬時に実行されるという仮定を排除してまいしたが、これでもまだ十分ではありません。やり取りするメッセージの数や流讃、サイズを増加することと、あるいは既存のネットワークに新しいプロセスを追加するときには、帯域幅は無限であるという仮定を置くべきではありません。

### 分散コンピューティングの八つの誤謬

1994年にPeter Deutschは、"Fallacies of distributed computing"（分散コンピューティングの誤謬）というタイトルの、今でも有名になっているリストを発表しました。これは、分散コンピューティングの背景として反証しがちな仮前提を述べたものの役は、誤は、ネットワークの信頼性、レイテンシ、および帯域幅の仮定のみではなく、その他いくつかの問題についても動べています。たとえば、ネットワークのセキュリティ、裁教を持つものの存在する可能性などの問題ですが、さらには、それのプロトコルのリソースの存在との両者に関する価値を明可能性がある、無同的および意図的ではないネットワークの変更の、時間ゆリソースの両面から既応な追加コスト、なぜこにネットワーク全体を把握し、制御する単一の権威の存在などを挙げています。

Deutschが発表した分散コンピューティングの誤謬のリストはかなり網羅的なものですが、1つのプロセスから別のプロセスに、リンクを介してメッセージを送信するときに発生する可能性がある問題に焦点が当てられています。これらに関心を持つのは妥当であり、一般的で既にレベルにおける基礎を指す単純に活を続けています。しかし、残念なことに、分散システムの設計および実装により思考すべき、実際のオベレーションで問題となり得ることは、他にも数多く存在します。

## プロセス

リモートプロセスが受信したメッセージに応答するには、その前に一定の処理をローカルで実行する必要があります。そして、**プロセス内処理は瞬時に行われると考えることはできません**。リモートプロセスによって実行される処理も、即座に行われるわけではないので、ネットワークのレイテンシを考慮に入れるだけでは不十分です。

そのためメッセージが配信されるとすぐに処理が開始されるという保証もありません。メッセージは、リモートサーバー上のキューに入れられて保留中となるかもしれません。その場合は、その前に到着したすべてのメッセージが処理されるまで待機しなければなりません。

ノードは、いいに近くに配置できて、異なるCPU、異なる容量のRAM、異なるディスクを搭載可能で、異なるバージョンおよび情報のソフトウェアを実行できます。それらが同じ速度で要求を処理することは期待できません。並行に動作する複数のリモートサーバーが、タスクの完了を位答して来るのを特だけなければならない場合、全体の実行として、もっとも遅いリモートサーバーに対応することになります。

広く信じられていることに反して、キューの容量は無限ではなく、多くの要求を積み上げるのはシステムに良い効果をもたらしません。バックプレッシャは、コンシューマが処理できる速度よりも速くプロデューサがメッセージを発行する場合に、プロデューサを減速することで対応を取るための戦略です。バックプレッシャは、分散システムにおいて正当に斉唱されておらず、適用されていない戦会の1つです。多くの場合、システム故障の必要不可欠な部分としてではなく、事後の対応策として利用されています。

キューのサイズを増やすことは、良いままえだと思われるかもしれません。さらに、要求をパイプライン化することにも、効果的なスケジューリングには有用かもしれません。しかし、メッセージがキューに配信され、処理される順番を待っている間は、それらのメッセージに対して何も処理が行われません。キューのサイズを増やすことは、レイテンシに対してマイナスの影響をおよぼす可能性があります。キーのサイズが変遷しても、処理速度にはったく影響がありません。

一般的に、プロセスローカルなキューは、以下の目標を達成するために使用されます。

### キューの役割

**分離**
受信とプロセス内でのる処理の時間的に分離され、それぞれを独立して実行されます。

**パイプライン化**
異なるステージの要求は、システムの独立した部分で処理されます。メッセージを受信する機能を実たすサブシステムは、前のメッセージが完全に処理されるまでブロックする必要がありません。

**短時間のバーストの吸収**
システムの負荷は変化する傾向があります。しかし、要求が到着する際のインターバルは、要求を処理するコンポーネントから隠されています。システム全体のレイテンシは、キュー内で費やされる時間のために増大しますが、応答としてエラーを返送するより、このほうが通常性は望ましいでしょう。

キューのサイズは、ワークロードやアプリケーションに固有です。比較的安定したワークロードの場合には、タスクの処理時間と各タスク処理される順番にキューに費やす平均時間を計測し、それぞれのサブシステムを配慮できます。の場合、キューのサイズは比較的小さくなります。予測不可能なワークロードに対しては、タスクがバースト状態で要求されたときに、バーストと称命を同時に受容して、キューのサイズを決定する必要があります。

リモートサーバーは、要求を処理時間で処理し続きることが印可能ですが、それは、常に新規的な応答を得られることを意味するわけではありません。障害が発生したという必答を返すこともあります。それは、書き込みができなかった、検索対象の値が存在しなかった、パケが発生した、などという場合です。要約すると、もっとも好都合なシナリオでさえ、本当の立場からは注意を要する余地があるとなります。

## クロックと時間

時間なんてど知想だ、特にお昼休みなんて。

—— Ford Prefect "The Hitchhiker's Guide to the Galaxy"

リモートマシント1のクロックが同期していることを仮定するのも、また危険です。これがレイテンシはゼロであると同じかけ合わせると、特に時系列データを処理やリアルタイムアークを処理において、異なる時異体に3つながります。たとえば、異なる時刻設値を持つ参加ノードからデータを収集および集計するときは、データもとのタイムスタンプに依拠するのではなく、それらの間の時刻のズレを調整し、それに従って時刻を正確化する必要があります。対象を両実度のデータフローを使用し仮想り、タイムスタンプに依拠して同期や順序付けを行うべきではありません。もちろんこれは、時刻に依存することはまったくできないし、そうすべきさもないというわけではありません。結局のところすべての同期システムでは、タイムアウトに**ローカルなクロック**を利用しています。

プロセス間の時刻の差や、メッセージの配信や処理に必要な時間を常に考慮に入れることが重要です。たとえば、Spanner（p.279「13.5 Spannerによる分散トランザクション」）では、タイムスタンプと不確実性の限界を返す特別な時刻APIを使用して、厳密にトランザクションの順序を定めています。それにより、一部の処理を待つ必要性を調整することで、長年された時間の概念とクロックのズレ（drift）が、常に許容される制限内で証整を使っているという保証に依存しています。GUPTA01。

分散システムにおけるクロックの問題は**時年の時刻**は議さ事雑化しています。現在のPOSIXタイムスタンプをオペレーティングシステムに問い合わせできますが、いくつかのステップの実行が可能な複数のタイムスタンプを開い合わせると、それら2つは異なります。これは結果として当然ですが、時刻をどのソースから取り出したのかの、タイムスタンプが厳密にどの時間の度を扱えたものかの両方を理解することは、極めて重要です。

また、クロックのソースが手方向かどうか、つまり逆方向には進延をないかどうか、およびスケジュールされた時刻に関連する操作が、どのるいえる可能性があるのかを理解することにも役立ちます。

## 状態の一貫性

これまでに挙げた前提の大半は、ほとんど常に偽り（almost always false）というカテゴリに属することが判明しましたが、**常に真実とは限らない**（not always true）として説明したほうが適切なものもあります。近道をしたらよか簡単などきには、特定の方法でそのことを考え、トリッキーなエッジケースを無視して、モデルを単純化します。

分散アルゴリズムは、厳密な状態の一貫性を常に保証するわけではありません。中には制限のゆるいアプローチもあり、レプリカ間の状態の不一致を許容する場合もあります。そのようなアプローチは、**競合の解決**（システム内で一致していない状態を検出して解決する機能）および、**読みとり修復**（レプリカが異なる状態を返した場合に、読み取り時に同時図に呼ばれた修復の、**もし考え方同じデータ修復**（レプリカがある状態を返桂した場合、読み取り時に同期区域に於ける修復）、このモードの更新データを強制し、それらの状態を解消させる）に依拠しています。これらの概念は、関する詳しい情報については、12章を参照してくださぃ。すべてのノードで、状態には全金を一貫性があると仮定すると、微妙な状況によも読み取り制限が交際します。

読み取り昭和を持つ分散データベースシステムには、読み取り時にノードのクォラムに問い合わせることで、レプリカの不一致を解消するロジックが含まれていることがありますが、データベースのスキーマとクラスタのビューに、強制な一貫性があることを前提にしています。この情報の一貫性を強固的に確保しない場合は、深刻な影響を指します。その不一致を解決しないは、クォラムのルール等の議決を使用してモデルを単純化します。

たとえば、Apache Cassandra（https://databass.dev/links/46）には、スキーマの変更が、各サーバーに異なるタイミングで伝播するという事柄によってき起こされるバグがありました。スキーマが伝播されている間にデータベースからの読み取りを行おうとした場合、データが吸取される可能性があります。このような事柄が何層かあるスキーマを前提にして解放をエンコードしましたが後て、別のサーバーが異なるスキーマを使用してそれらをデコードしていたためです。

もう1つの例としては、リングのビューの不一致（https://databass.dev/links/47）によって引き起こされるバグがありました。あるノードでは、別のノードが一に対応するデータレコードを管理していることと想定していたのに対し、他のノードでは、異なる状態でクラスタを認識している場合がありうます。こうした古口に、データの読み取りまたは書き込みは、データとコードを礁にした適所に配慮したり、実際にはデータレコードが他のノードに存在するといつ結果を招くことがあります。

考えうる問題については、たとえ完全なソリューションを実装するのに大きなコストがかかるとしても、前もって検討しておいたほうが賢明です。これらのケースを理解して対処することによって、より堅牢なものとして、ソリューションに安全対策を組み込んだり広告を変更したりすることが可能になります。

## ローカル実行とリモート実行

API の背後に置換きを隠蔽するのは、危険を伴う場合があります。たとえば、ローカルのデータセットに対してファイルシステムから読み取る操作と、たとえストレージエンジンに不備住ぁらべも、背景でのような処理が行われているかを予測できるでしょう。ただし、リモートのデータセットに対するイテレーションのプロセスを理解することは、まったく別の問題です。ここで理解する必要があるのは、一貫性と配信の考え方、データの識別、ページング、マージ、並行アクセスの意味合い、および差の他多くのことです。

単純に同じインターフェイスの背後に両者を隠蔽することは、いかに便利であっても、詐欺を狙うおそれがあります。デバッグ、構成、可観測性（observability）のためには、APIには追加パラメータが必要になります。**ローカル実行とリモート実行は同じではない**ことを常に念頭に置いておかなければなりません [WALDO96]。

リモート呼び出しを隠蔽する際のもっとも明白はレイテンシです。リモート呼び出しは、シリアライズ／デシリアライズといら双方向のネットワーク伝送を含む、多くのステップを必要とするので、ローカル呼び出しよりも遙信がコストがかかります。ローカル呼び出しをインターリーブしたり、リモート呼び出しをブロックすることは、パフォーマンスの劣化や予期しない副作用につながる可能性があります [VINOSKI08]。

## 障害への対処の必要性

すべてのノードが起動されていて、正常に機能していることを前提にシステムを使用し始めるのは良いですが、システムを長期間稼働している場合、ノードがメンテナンスのために停止されたり（この場合は通常、正式なシャットダウンが要求されまます）、ソフトウェアの問題、メモリ不足による一部のプロセスの強制終了（out-of-memory killer）[KERRISK10]、実行時のバグ、ハードウェアの問題など、さまざまな理由でクラッシュすることがあります。プロセスには障害が付きものので、最良の方策は、障害に対する準備を整え、それらの対処方法を理解しておくことです。

リモートサーバーが応答しない場合に、その正確を理由が何にわかるとは限りません。その状況を引き起こしたのはクラッシュかもしれませんし、ネットワークの障害やリモートプロセス、あるいはスレッドの圧迫化的もしれません。いずれの場合も、障害の応答があり得ます。前の分取りアルゴリズムでは、ハートビートプロトコルおよび**障害検出**を使用して、どの参加ノードが動作中でアクセス可能についての概念を立てます。

## ネットワーク分断と部分障害

2つ以上のサーバーが相互に通信できなくなったときに、その状況はネットワーク分断と呼ばれます。Seth GilbertとNancy Lynchは、"Perspectives on the CAP Theorem"[GILBERT12]において、2つの参加者が相互に通信できない場合と、いくつかのグループの参加者が相互に分断されてメッセージを交換できない状態で、アルゴリズムに従って処理を進める場合の違いを説明しています。

ネットワークの一般的な信頼性の低さ（予測が困難なパケットロス、再送、レイテンシ）は、迷惑ですが容認範囲です。それに対してネットワーク分断は、独立したグループが実行を続行することは不可能になるため、さらに大きな問題を引き起こしかねません。また、ネットワークリンクは、非対称に障害を発生させることがあります。つまり、メッセージを1つのプロセスから別のプロセスに配信することは可能なのに、その逆向きには配信できないことがあります。

1つ以上のプロセスに障害があっても正常に動作し続けられるシステムを構築するには、**部分障害**[TANENBAUM06]のケースを思し、システムの一部が利用できなかつたり正しく機能しながつたりしても、どのようにオルエンドシステムを通州し続けられるかを検討する必要があります。

障害は検出するのが困難であり、システムの異なる脳部から常に同じように見えるとは限りません。可用性の高いシステムを設計するとぎには、必ずエッジケースについて考えなければなりません。
