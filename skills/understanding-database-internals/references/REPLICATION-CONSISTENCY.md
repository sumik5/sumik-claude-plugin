# レプリケーションと一貫性

第11章「レプリケーションと一貫性」のリファレンス

## 可用性の実現

本書では、これまでに分散システムの諸課題について紹介し、問題が発生する原因となる多くのことを確認してきました。現実の世界では、ノードがいつでも正常に動作したり、相互に通信できたりするわけではありません。しかし、断続的な障害によって**可用性**が影響を受けてはなりません。ユーザーの観点から見れば、システムは全体としては同じ振る舞いをすいないかのように動作し続ける必要があります。

システムの可用性は、非常に重要な特性です。ソフトウェアエンジニアリングにおいては、常に高い可用性を目指し、ダウンタイムを最小にするための試みが行われています。エンジニアリングチームは、その稼働時間の指標を日

開に褒し、可用性について、それはどさくの性善を払うか

は、いくつかの理由の中から、一番の理由は必ずしも利益取り、通信、旅行などの、ソフトウェアなしには接続できなくなっているからです。

企業にとって可用性の欠如は、組宮やお金を失うことを意味しますが、オンラインストアがダウンすれば建ゃがなくなりますし、銀行のウェブサイトが応答しなくなければお金を振り込むことができなくなります。

システムの高い可用性を維持持するには、1つまたは複数の参加ノードの障害や、利用不能などのケースに、未然に対応できるように設計する必要があります。そのためには、冗長性とレプリケーションを導入することが必要があります。ただし、冗長性を導入するということに、データの複数のコピーを同期させるという問題に直面し、リカバリのメカニズムの実装が要求になります。

## 悪名高いCAP

**可用性**は、すべての要求に対して、正常に応答を提供するシステムの能力を測る特性です。可用性の理論的な定義としては、最終的に応答できることを意味しますが、もちろん、現実世界のシステムでは、応答に膨大な時間がかかることは許されません。

理想的には、すべての操作には**一貫性**があることが望まれます。ここでの一貫性は、アトミックな一貫性、または**線形化可能性**一貫性と定義されます（p.230「11.5.2 線形化可能性」）。線形化可能な属性は、もとの操作間序を保ったまま瞬間的に行われる一連の操作として表すことができます。線形化可能性によって、取り得るシステムの状態に関する推論が単純化され、分散システムは単一のマシン上で実行されているかのように見えます。

単一で可用性の両方を選択できる一方で、ネットワーク分断が存在するとそうーーはいきません。ネットワークはいくつかの部分に分割されて、プロセスが相互に通信できなくなる可能性があります。分断されたノードの間で送信されたメッセージの中には、その宛先に到達しないものも出てきます。

可用性は障害のないノードが結果を返すことを求め、一貫性はその結果が最新であることを求めます。Eric Brewerによって定式化されたCAP定理では、一貫性（Consistency）、可用性（Availability）、および分断耐性（Partition tolerance）のトレードオフについて議論しています [BREWER00]。

可用性の要件を、非同期システムにおいて満たすことは不可能です。さらに、ネットワーク分断が存在する中で、可用性と一貫性の両方を同時に保証するシステムを実装することはできません [GILBERT02]。できるのは、強力な一貫性またはベストエフォートの可用性を提供するシステム、または可用性を重視するシステムを構築することです。この場合の、ベストエフォートが意味するのは、すべて正常に機能している場合には、システムが意図的に保証を検供するということはないが、ネットワーク分断が発生した場合には、保証を前誰たりリラックスしたりすることが許容されるいうことです。

つまり、CAPでは一連の取り得る選択肢を説明しており、その範囲内で、以下のような特性を持つシステムを実装します。

### 一貫性と分断耐性を備えたシステム

CPシステムは、一貫性が失われた可能性のあるデータを提供する場合には、リクエストを失敗させます。

### 可用性と分断耐性を備えたシステム

APシステムは、一貫性の条件を順化し、リクエストに対し、一貫性が失われた可能性があるデータを提供します。

CPシステムの例としては、処理を進めるために過半数を必要とする合意アルゴリズムの実装があります。これは有在に一貫性がありますが、ネットワーク分断が発生した場合には、可用性を失うことがあります。また、1つのレプリカが動作中である限りは、常に書き込みを受け入れ、読み取りに応じるデータベースは、APシステムの例になります。これは、データを実装したり一貫性の提供がないが結果を返したりすることがあります。

PACELEC定理 [ABADI12] はCAPを拡張したものですが、それによると、ネットワーク分断が発生すると、一貫性と可用性の間で選択が行われると述べられています（PAC）。それ以外の場合には（E）、たとえシステムが正常に動作していたとしても、またレイテンシと一貫性の間で選択を行う必要があります。

### 注意が必要なCAPの使用

CAPにおいて、ノードのクラッシュやその他のタイプの障害、たとえばクラッシュリカバリなどを定義されていることに、注目に値します。ノードは、通常には障害しませんでした。1つの見方としては、いずれかのノードがダウンしていることは、注目に値します。ノードは、ランクを高いとしては応答しなかったのですが、クラッシュした場合にはまったく応答しません。1つの対処法と実装として実はまったく異なりますが、CAPクラスの他の部分から分断されても一貫性のないリクエストを提供できますが、クラッシュした場合にはもっとなく意答しません。一部のノード障害やその隋のリクエストも受けられない

、さらに、CAP定理は、三者形としては関連されることがあります。それはまさぐ、つまを何1つことだけができは使ことになるのです

。ただし、つまを回して一貫性ことと、予满不可能を結果をもたらすこともできます

CAP定理は、分散システムを設計し、障害シナリオについて推論し、起こり得る状況を詳論するために使用されますが、一貫性を放棄することと、予満不可能を結果をもたらすこともできます。

可用性を提供することを主眼とするデータベースであっても、正しく使用されるときには、レプリカから一貫性のある結果を提供することが可能です。ただし、十分な数のレプリカが正常に動作していろという条件が満たされる必要があります。もちろん、さらに複雑な障害シナリオも存在するので、CAP定理は単に特性の妥を含たすべての実要を伝えるものではありません。

### 収穫と収量

CAP定理では、一貫性と可用性について、それらのもっとも強力を形式でのみで語られています。そのそとらのうち、**線形化可能性**と、すべてのリクエストに最終的には応答することです。

このために、2つの特性の間で、繭しいトレードオフを全般なくされます。ただし、アプリケーションの中には少し厳腿を機花しても構成されるものがあり、こうした特性については、それを比較的弱い形で考えることができます。

システムは、一貫性または可用性の代わりに、線和された民座を提供できます。**収穫**（harvest）と**収量**（yield）という2つの調整可能なメトリクスを定義できます [FOX99]。いずれが正しい動作を引き続き遺択するかを選択できるように、この2つを定義します。

### 収穫（harvest）

問い合わせどという反った応答する不完全答えます。あるい問い合わせは100行を返す必要がありますが、一部のノードが使用不可になっているので、99行しが取り出せなくとも、問い合わせが完全に失败して、何も返さないよりははしです。

### 収量（yield）

正常に完了したリクエストの数を、試行したリクエストの総数と比較して掃定します。収穫は、アップタイム（uptime）とは異なります。たとえば、高有朽なノードはダウンしているわけではありませんが、一部リクエストの応答に実敗する可能性があるからです。

これによって、トレードオフの焦点が、厳密的という言葉から、相対的という言葉に移ります。収穫を収量に入れ替えると、一部のリクエストが不完全なデータを返すなる許容できます。収穫を増大する方法の1つは、問い合わせの結果を利用可能なパーティションからのみ取得ことです [BREWER00]（p.281「13.6 データベースパーティショニング」）。たとえば、一部のユーザーのレコードを格納しているノードのサブセットがダウンした場合でも、他のユーザーのリクエストには応答し続けることが可能です。あるいは、重要なアプリケーションデータは完全な状態で応答することを要求できますが、他のリクエストには早くるかの選択が許容できます。

収穫と収量を定義し、調整し、意識的に選択することで、障害に対してより弾力性のあるシステムを構築することができます。

## 共有メモリ

クライアントから見ると、データを格納している分散システムは、単一ノードシステムと同様に、実有ストレージが独立実装されているかのように動作します。ノード間通信とメッセージの受け渡しは抽象化され、ユーザーには見えないところで実行されます。これにより、共有メモリがあるかのような錯覚を起こします。

読み取りまたは書き込みの操作でアクセスできるストレージの単一ユニットは、通常レジスタと呼ばれます。分散データベースに含まれる共有メモリは、そのようなレジスタのアレイであるとすぐに見えます。

すべての操作は、その**起動イベント**と**完了イベント**によって識別されます。ある操作を起動したプロセスが応答する前にクラッシュした場合、その操作は**失敗した**と定義します。1つの操作の起動イベントが特定の瞬間に対応する前に、別の操作が完了した場合には、この2つの操作は**並行**していると言い、これらの2つの操作は**シーケンシャル**であると言います。それ以外の場合には、それらの操作は先行すると言い、これらの2つの操作は**シーケンシャル**であると言います。それ以外の場合には、それらの操作を**並行**していると言います。

図11-1を見ると、プロセスP₁とP₂が異なる操作を実行している状況を確認できます。

- a) プロセスP₁によって実行される操作は、P₁によって実行される操作がすでに**終了した後**で開始されます。つまり、2つの操作はシーケンシャルです
- b) 2つの操作はオーバーラップしています。したがって、これらの操作は**並行**しています
- c) P₂によって実行される操作は、P₁によって実行される操作が後で開始され、その前には完了します。これらの操作も**並行**しています

複数の読み手（reader）、または書き手（writer）が、レジスタに同時にアクセスできます。レジスタに同じように操作を実行している間に、読み取り書き込みの操作が異なるプロセスによって並行して実行されると、それらは**リアル**ではありません。操作がオーバーラップすると、きのレジスタの寄稿によっては、それらの操作は異なる間序で並べられ、異なる結果を生成することもあります。並行操作があった場合のレジスタの動作に応じて、以下に示す3つのタイプのレジスタを区別します。

### セーフ

セーフレジスタへの読み取りは、並行書き込み操作のレジスタの値開閉に含まれる**任意の値**を返すことがあります。これはあまり実用的ではないと思われますが、制序を満載しない非同期システムの文脈では許容される可能性があります。ハイオナリ値を持つセーフレジスタは、許さ込みと並行する読み取りの間に、つまり2つの値の間で交互に変わる結果を返すように見えることがあります。

### レギュラー

レギュラーレジスタでは、もうがし強力な保証が得られます。読み取り操作は、最後に**完了した**書き込みによって書き込まれた値か、あるいは現在の読み取りとオーバーラップしている書き込み操作によって書き込まれた値のどちらかだけを返すことができます。この場合は、古い値を返すことまたは新しい値を返すかのどちらかであるものの、書き込み操作が異なる値を単複混じえることありません。たとえば、これはレプリケートされたデータベースで発生することがあります。レプリケートされたデータベースでは、プライマリに書き込みを受け入れて、読み取りを行うワーカーに対してそれらをレプリケートします。

### アトミック

アトミックレジスタは、線形化可能性を保証します。すべての書き込み操作には、ある単一の瞬間があります。この瞬間の以降は行われる読み取り操作がすべて新しい値を返し、その後はすべての読み取り操作が新しい値を返します。アトミッ

ク性は、システムの状態に関する推論を単純化する重要な特性です。

## オーダリング

一連のイベントを見るときには、それらの実行順序について論理的に読み解くことができます。ただし、それはど単純であるとは限りません。なぜなら、何らかの動作が**厳密**に以実行されたかを知り、この情報に照して、クラスタ全体で利用できるようにするのは困難だからです。各参加者は、状態についてそれぞれのビューを持つことがあります。しかし、現実の単独なものを見方から見た地場合の妥当方です。読み取り操作は副作用を訓読させません。

ここで、プロセスがread(register)とwrite(register, value)という操作を、共有レジスタに対して実行できるシステムを定義しましょう。各プロセスは、それ自体の一連の操作を一シーケンシャルに実行します。そのため、書き込まれた後ですぐすべて完了しない、次の操作は開始できません。シーケンシャルなプロセス実行を組み合わせると、グローバルな複雑が形成され、その中では操作を並行して実行できます。

一貫性モデルについても、もっとも単純な変え方は、読み取りと書き込みの操作、およびそれらをオーバーラップさせる方法から理解を始めることです。読み取り操作には副作用がありません。

が、書き込み操作はレジスタの状態を変更します。このことから、書き込みの後でデータが実際にいつ読み取り可能になるかを推論できるようになります。たとえば、2つのプロセスが以下のイベントを並行して実行する敵鋭について検討します。

```
Process 1:       Process 2:
write(x, 1)      read(x)
                 read(x)
```

これらのイベントを見ると、どちらの場合も、read(x)操作の出力がどうなるかが不明確です。生成される可能性のある敵艦は、以下に示すように数通りあります。

- 書き込みは、両方の読み取りの前に完了します
- 書き込みと2つの読み取りはインターリーブされ、読み取りの前に実行されます
- 両方の読み取りは、書き込みの前に完了します

データのコピーを1つしか持たない場合に起きることについて、単純な答えは存在しません。レプリケートされたシステムでは、可能性のある状態の組み合わせの数が多くあり、データの読み書きを行う複数のプロセスがあるときには、まずまず複雑になる可能性があります。

これらの操作がすべて単一のプロセスによって実行される場合には、イベントの版意の実行順序を決定することが可能です。しかし、プロセスが複数ある場合には、それは困難になります。この点在在名に関係る候補をは、以下2つのグループに分類できます。

- 操作は、オーバーラップする可能性があります
- オーバーラップしない重け出しの影響は、即座には見えない可能性があります

操作の順序について推論し、可能性のある結果を降順にぜず正説明するには、一貫性モデルを定義する必要があります。分散システムにおける並行性について、共有メモリと並行システムの間で用語の多くが重複するとしても、通信パターン、パフォーマンス、および信頼性においては差異があり、並行性のアルゴリズムの大半を直接我用することはできません。

## 一貫性モデル

共有メモリのレジスタ上での操作がオーバーラップすることは許容されます。そのため、複数のクライアントが同時に、またはを時間内に、データの異なるコピーを読み取ったり変更したりした後に、何が起きるかいうことを明確に定義しておく必要があります。この問題に対する唯一の正しい答えはありません。起きたことに対する意味付けは、アプリケーションによって異なるからです。次に研究されていきます。

**一貫性モデル**は、多様な意味付けと保証を提供します。一貫性モデルが、参加者の間の契約であると考えられます。つまり、要求されか内容を満足させるように合もレプリカが行わなければならないこと、ユーザーが望まれる択を上げることを見します。

単純な動作行では、データへの並行アクセスが既整数化したり、そのデータへの並行アクセスが存在しながら、どうしても遅元的要求を提供することを要求します。このセクションでは、**単一操作**の一貫性モデルについて議論します。

集全ては、読み込まそのイベントがいくつかの問いと思う規区類いがり、当然として出力連晃と思う撹議とモデル文の下で許容される敵艦（これによって、状態の変更の可能性についての推論が大幅に単純化されます）を区別するために役立ちます。

一貫性モデルについては、異なる実け出しされかであり、当然と出る規区に達い方分けるとができます。さらに、この状態性が許容されるかを説明できと、異なるレプリカ上に格納されたデータのコピーの間で許容される関間を確認できます。あるいは、操作の一貫性について考慮することもできます。これは、データストアに対する外部ビューを提供し、操作を説明し、操作が行われる順序に制限を課します [TANENBAUM06] [AGUILERA16]。

後者のアプローチのほうが直観的で使いやすいです。クライアントがレジスタに書き込んだり、その値を読み取ったりする操作を実行するとき、グローバルなクロックがない場合、分散操作に対して正確で決定的な順序を通用するのは困難です。それは、データに対して外部から特性が適用することが許可されるのです。すべての参加者が、状態と時間の概念を持っています。

理論上は、システムの入的の判断を問連すことはできません。グローバルな敬虔な状態を定儀するた ませには、システム内の障害的拡拉をテストするような必要があります。すべての参加者が、状態認識帯を共有している必要があります。

現在モデルは、p.222「11.2 悪名高いCAP」で検討した内容に沿って、今の次元を追加します。今や、一貫性モデルに関する状況についても必ずエッジケースを考えなければなりません。CAP定理を、思いのままなコストからら一貫性について考慮することと区別する系口として使用します。今や、同期コストには、レイテンシ、追加の複雑化に費やされる追加CPU サイクル、リカバリ情報をディスク上で永続化するために使用されるディスクI/O、待ち時間、ネットワーク分断なとをはじめとする可能性があります。

最初に、操作結果の可視性と伝播に焦点を当てます。並行して実行される読み取りと書き込みの例を再度取りあげると、仮存する書き込みの後に書き込みが到着した場合、または新しい値が伝播される前にポイントを定義することで、取り得る両座の数を制限できます。

ここでは、データベースの状態に対して、read操作とwrite操作を実行するプロセス（クライアント）の観点から一貫性モデルを議論します。レプリケートされたデータについての一貫性を議論するので、データベースは複数のレプリカを作成できるとします。

### 厳密な一貫性

**厳密な一貫性**は、レプリケーションの完全な透過性と同義です。プロセスによる任意の書き込みは、その後に続くすべてのプロセスによって即座に可視です。これはグローバルなクロックの概念を含まれており、ある瞬間にwrite(x, 1)が実行された場合、read(x)は、任意の瞬間で新しく書き込まれた値を返します。

完全ながらこれは単に理論的なモデルにすぎず、実装するのは不可能です。というのは、物理法則とから、瞬間的な伝播が発生する進言には限慎期に行われると述べることは [SINHA07]。

### 線形化可能性

**線形化可能性**は、単一オブジェクト、単一操作のもっとも弱力な一貫性モデルです。このモデルが示では、書き込みの内容は、何部が完了してから即座に正常に可視ですら、クライアントは状態の推移を観慮できません。クライアントは、状態の推移を見えるようになります。クライアントは、部的が、つまり未完全に、またれ不完全に、つまり充了順に害き込まれたとしても、書き込み操作の副作用は、原子性が保証される操作の順序に実行されます [HERLIH94]。

並行作用は、原子的特性が保持される、取り得るシーケンシャルな服の1つとして表現できます。イベントの順序付けには複数の方法が存在するので、線形化可能性には不明瞭な部分があります [HERLIHY90]。

2つの操作がオーバーラップする場合、どのような順序でも意図した効果を生成できます。書き込み操作が完了した後で行われるすべての読み取り操作は、この書き込み操作の結果を見ることができます。ある読み取り操作がある特定の値を返すと、その読み取り操作以降に実行されるすべての読み取りは、少なくともそのそれらの読み取り操作は読みし又ここのの値を返します。すなわち、値が読み取られた後、それを前に実行された価を返されることはありません。

最近のプログラミング言語の大半では、アトミックなwrite（書き込み）と、コンペアアンドスワップのような他のアトミックなプリミティブが提供されています。アトミックなwrite操作では、前の値が変化していない場合にのみ名前参陽がの変更されるCAS と呼条なひ9、呼氏のレジスタの値比と考蔵しませし[HERLHY94]。 CASに多様な読み取り条件になからえず、呼氏の2つの近行誌き込みによって、値目が書き込まれてた次に読読に確認されなければ、維新な変取付としても、値Aが存在することになりっは、最後の読み取り取引に個が作出されていないことは保証されはしません。

線形化ポイントは1つの区切りとしての役割を果たし、その後は操作の効果が可視化されます。これは、重要な局面を保護するロック、アトミックな読み取り/書き込み、またはリードモディファイライト（read-modify-write）のプリミティブを使用することで実装できます。

図11-3は、線形化可能性が維数的時間的場界を前提とし、時刻はリアルタイムであることを示しています。操作の効果は、操作の要求を発行したもしもと、応答を受け取ったものの間で可視化される必要があります。

図11-4は、線形化ポイントが順序を前と後に分割する様子を示しています。線形化ポイントの前ではけい納が見えていて、線形化ポイントの後では新しい値が可視化されています。

#### 線形化可能性のコスト

多くのシステムでは現実、線形化可能性の実装を避けています。CPUにおいてさえ、アップデートをインメモリに行う操作とき、線形化可能性は提供されません。同期化命令がないストが高く、速度が落なく、ノード間に渡るCPUトラフィックとキャッシュの無効化が必要だからです。ただし、軽いレベルのプリミティブを使用して線形化可能性を実装することは可能です [MCKENNEY05a][MCKENNEY05b]。

並行アルゴリズムのフルでは、結果を用意し、次にCASを使用して線形化可能性を導入できます。多くのアルゴリズムは、結果を計算し、次にCASを使用してポイタを交換し、それらを公開することで動作します。たとえば、連結リストノードを作成してから、それをリストの末尾にアトミックに追加することによって、並行キューを実装できます [KHANCHANDANI18]。

線形化可能性には、高額となるコストがかかります、それは、合意を使用して実装できます。クライアントは、レプリケートされたストアとメッセージを用いて対話し、合意モジュールは、適用された操作がクラスタ全体に渡って一貫性を持ち、同一であることを保証する機能を持ちます。それぞれの書き込み操作は、その起動イベントと完了イベントの間の、ある瞬間が順序付けられます [HOWARD14]。

興味深いことに、従来の理解では、線形化可能性は**ローカルな特性**であるとみなされ、個々に実装されて保証された要素の組み合わせであることを意味します。複数の線形化可能な服装を結合すると、結果のものオブジェクトも線形化可能になります [HERLHY90]。つまり、すべてのオブジェクトが線形化可能であるシステムは、そのシステム自体も線形化可能だということになります。これは非常に便利な特性ですが、その有効範囲が単一のオブジェクトに限られることを忘れてはなりません。2つの独立したオブジェクトに対する操作が線形化可能だとしても、2つのオブジェクトにかかわる操作は、現同の同期手段を通りとする必要があります。

#### RIFL

RIFL（Reusable Infrastructure for Linearizability、線形化可能性のための再利用可能なインフラストラクチャ）は、線形化可能なRPC（リモートプロシージャーコール）を実装するためのメカニズムです [LEE15]。RIFLでは、クライアントIDとクライアントローカルで単調増加のシーケンス番号を使用して、メッセージを一意に識別します。

メッセージ番号がすでに見識されるように、RIFLではリースが使用されます。これはシステム全体で共有されるサービスによって発行される一意の識別子であり、一意性を確定し、シーケンス番号の競りを排除するために使用されます。障害が発生したクライアントが、有効期限のきれたリースを使用してこの操作を実行しようとした場合には、コミットされません。クライアントには、新しいリースを受け取って再送信する必要があります。

サーバーが、書き込みに対する確認応答を行う前にクラッシュした場合、クライアントは、それがすでに適用されていることを認識せずに操作を再送信しようとするかもしれません。クライアントがC1が関いV1をC2が書き込み、そして間に、クライアントにどリ操作が失敗したという状況があり得ます。その間に、クライアントはC2が前いV2を書き込んだとします。C1がその操作を再送信した場合、C2の書き込みは失われます。このような事態を避けるために、システムは再済行された操作が再度実行されることを防止する必要があります。クライアントが操作を再送信すると、操作はコミットされるわけりに、RIFLは完了オブジェクトを返送し、関連付けられている操作はすでに実行済みであることを示し、その結果を返送します。

完了オブジェクトは、実際のデータレコードとともに、永続可能なストレージに格納されます。ただし、それらの寿命は異なります。完了オブジェクトが存在しなければの、発行元のクライアントが関連はいと想起されるまで か、あるいはクライアントがクラッシュしたことをサーバーが検知するまでです。このような場合、関連付けられた完了オブジェクトは、すべて安全に削除できます。完了オブジェクトの作成は、関連付けられたデータレコードの変更とアトミックでなければなりません。

RIFLのリース、完全性を保証できるために、定期的にリースを更新する必要があります。クライアントがその

リースの更新に失敗すると、クラッシュしたというマークを付けられ、そのリースに関連付けられているデータはすべてガベージコレクションの対象とされます。リースの寿命は調整されています。これは、失敗したプロセスに関する操作が失次に書き込まれることがないようにするためです。失敗したクライアントが、消してクライアントが有効期限のきれたリースを使用してこの操作を実行しようとした場合、その結果はコミットされず、クライアントは成功からやり直す必要があります。

### 逐次一貫性

線形化可能性の達成は高くつくかもしれません。しかし、モデルを緩和し、分かり強力な一貫性を引き続き保証することが可能です。**逐次一貫性**（Sequential Consistency）は、操作があるシーケンスの中で、ほぼ逐次にも順序付けられているかのように、操作を順序付けできます。その一方で、個々のプロセスの操作は、そのプロセスが実行した順と同じ順序で実行されなければなりません。

プロセスは、他の参加者によって実行された操作を、それぞれの順度と一貫性のある順序で見ることができます。しかし、このビューは、リアルタイム順序とは偶に合致的なため見えると言えつづいている可能性はありません [KINGSBURY18a]。プロセス間の実行順序は、時間を共有する概念がないので、完全さを保つことができません。

逐次一貫性は、当初、並行性との関連性を前提として紹介され、マルチプロセッサプログラムを正規化する概念として定義されました。その限り、マルチプロセサプログラムの実行では、同じように対対するメモリ読み書きの順序がキューの中で割り当てられる（FIFO、着信順）ことが必要とされ、独立したメモリレベルへのオーバーラップする書き込みには、グローバルな順序付けを課していませんでした。また、読み取りでは、メモリセルの値、またはキューがて゛なかった場合には、キーがからの最新の値を取り出すことが可能でした [LAMPORT79]。しかし、この例は、逐次一貫性の意味を理解するうえで役に立ちます。著者観に依存したり、2つの書き込みが同時に着信したときには任意の順序にしたりできます。ただし、すべてのプロセスは操作を同じ順序で見ることになります。

プロセスでは、プログラムで指定された順序で読み取りと書き込みの要求を発行できます。これは非常に自然的です。並行していないシングルスレッドのプログラムでは、そのステップが1つまたは1つし順番に実行されます。同じプロセスから伝播するすべての書き込み操作は、このプロセスによって発行された順序で見えます。異なるソースから発行された操作は、任意に順序付けられることがありますが、読み手から見れば、この順序には一貫性があります。

逐次一貫性と線形化可能性は似たような意味ではい究付きのて、しばしば混同されます。逐次一貫性のスケジュールは、含有できせんか [ATTIYA94]。

図11-5は、write(x, 1)とwrite(x, 2)が、どのようにP₁とP₄には見えるようになるかを示しています。ウォールクロックの観点から見れば、1は2より前に書き込まれていたとしても、順序付けは2より後になかあります。同様に、P₄はまずC₁に値1を読み取っているに一方で、P4は土2を読み取ります。ただし、両方の順序、1 → 2と2 → 1は、異なる読み手から見て一貫性がある限りは存在です。ここで重要なのは、P₃とP₄の両方が、値を何回し順序で、つまり最初に2、次に1という順序で見たということです [TANENBAUM14]。

古いデータの読み取りは、たとえばレプリカの不一致で発生できます。たとえば書き込みが両立に順序付けされていない場合でも、他のプロセスに伝播しなければなりません。線形化可能性のもとでは、操作は、ウォールクロックの制限の中で結果が有効になる必要があります。書き込み操作型が完了するまでに、その結果は通信される必要があります。すべての読み手は、少なくとも論理に含まれる限度で見えることができるはずです。同様に、読み取りに、読み取りや書き込みに先んじて行われることができます（レレルに役立つ思かせん）を返すはずです。

一貫性は、これの要件を緩和します。操作の結果は、例々のプロセスの観点から見て順序付けられる必要があります。その他の一貫性は、その読果は通し再現できるように必要があります。すべての読み手は、少なくとも書き込みが存られて完了しなくてはならないと想よです。

線形化可能性と同様に、成新のCPUは、逐次一貫性をデフォルトとして保証しません。プロセッサは命令の順序を並び替えることがあるので、並行して実行されるスレッドに書き込み順序とおよび見えるえうにするために、メモリバリア（フェンスと呼ばれることもあります）を使用しなければなりません [DREPPER07][GEORGOPOULOS16]。

### 因果一貫性

この世で書溶的なことが1つだけある。それは唯一の本当の真実だ。つまり因果関係。作用、反作用、原因と結果。

—— 「マトリックスリローデッド」よりメロビンジアンの言葉

グローバルな操作の順序は、多くの場合において保持する必要がないとしても、**一部の操作間に順序を確立することは必要**な場合があります。因果一貫性モデルでは、すべてのプロセスが互いに同じ順序で見えなければなりません。因果関係のない**並行書き込み**は、異なるプロセサから異なる順序で見られてもかまいません。

まず、なぜ因果関係が必要なのか、そしてどのようにして因果関係のない書き込みを伝播できるかについて確認します。図11-6では、プロセスP₁とP₂が因果関係に**順序付けられない**書き込みを行っています。これらの操作の結果は、異なるタイミングで異不同で読み取られます。プロセスP₃は、値2を見る前に1を見ています。それに対してP₄は、先に2を見てから1を見ています。

図11-7は、因果関係のある書き込みの例を示しています。ここでは、書き込まれた値に加えて、操作間の因果関係に基づく順序を確認する論理クロックの値も拡定する必要があります。P₁は、はじめに書き込み操作write(x, t1, 1)→t₁を実行します。この操作は、論理クロックの初期値ののから始めます。P₃は、もう1つの書き込み操作write(x, t1, 2)を実行し、これが論理的にはその後に確認応答を送信することを指定し、さらに論理クロックによって確立された順序での改併作性を伝播されることを表求します。

これにより、上述の操作の間に**因果関係がある関係**が確立されます。後者の書き込みが前が書き込みより遅く送されたとしても、それに依存するもの件ずべて対象し、それらの論理的なタイムスタンプからイベントの順序が中律率されるまでは、可視化されません。言い方を変えると、happened-before の関係が、物理的なクロックを使用せずに論理的に確立され、すべてのプロセスがこの順序に同意します。

図11-8は、プロセスP₁とP₂が因果関係のある書き込みを行い、それが論理的な順序でP₃とP₄に拡布する様子を示しています。これは、図11-6に示した状況とならことを防ぐことができます。両方の例を見れば、P₃とP₄の服欠を比較できます。

これは、あるオンラインフォーラムでの通信にたとえて考えられます。あなたが何かをオンラインに投稿すると、別の読み誰がとそり投稿を見る機会があり、さらに3人目の人がその応答を見る、というようにして会話スレッドが生成される場合には、分岐することも可能です。つまり、スレッドに含まれる会話の中の1つを選択し、そのイベントの連鎖を統けることができます。しかし、いくつかのスレッドに並測するメッセージがけすずすしかない場合もあるので、すべてのメッセージに対する単一の順所は存在しない可能性があります。

あるメッセージの要求と、それに対する応答には、一貫性がない可能性のある異なるサーバーに対して実行したとしても、データベースのビューがこれ自体の動作との一貫性を保つっえうすれば、アプリケーションに対するセッションの保証が得られます [TERRY94]。これらの保証の対象は、モノトニックな読み取り、モノトニックな書き込み、書き込み後読み取り（writes-follow-reads）です。これらのセッションモデルについて詳しくは、p.240「11.6 セッションモデル」を参照してください。

因果一貫性は、通理クロックを使用して [LAMPORT78]、メッセージごとにメタデータを送信し、どの操作の現在の操作より高順位的に発行するかを示すことで実装できます。サーバー間の同期は、すべてのパーティションが含まれます。あるある操作は、それに先行する前にすべての操作がすでに適用されている場合にのみ、処理できます。そうした処理に適合しないメッセージは、実行するには時期尚早なので、サーバー上でバッファリングされます。

因果一貫性を提供するシステムの1つが、データベース、著名で知累に引用される2つの、COPS（Clusters of Order-Preserving Servers）[LLOYD11]と、Eiger [LLOYD13]です。これらのプロジェクトは、両方とも、ユーザーが接続するフロントエンドサーバーとして実装されたライブラリを介して因果関係を実装し、一貫性を確保するために長存価値を追跡します。COPSでは、キーバージョンを使用して長存価値を追跡しますが、一方のEigerでは、その代わりに操作間序を確立します。Eigerにおける操作は、他のノード上で実行される操作に依存する可能性があります。たとえばマルチパーティションのトランザクションの場合などです。どちらのプロジェクトも、輸送性のデータストアとは異なり、順不同の操作を可視化することはありません。その代わりに、緩和を検出して処理します。COPSでは、これを実行するためにキーオーダーをチェックし、アプリケーション固有の構成を使用します。それに対して Eigerでは、最後の書き込みが前のールール（last-write-win）を実装しています。

#### ベクタークロック

因果関係による順序を実現することで、システムでは、メッセージが順不同で配信された場合でもイベントの順序を再構築し、メッセージ間のギャップを埋め、一部のメッセージが欠落している場合には操作結果の可視化を回避できます。たとえば、メッセージ{M1(0, t1), M2(M1, t2), M3(M2, t3)}はそれぞれを低存関係を持定しており、これらが因果順序を持ちながら順不同でて到着し、すべての操作の低存関係を把握し、因果関係による順序を使用できるまでこれらをバッファリングします [KINGSBURY18b]。多くのデータベース、たとえば、Dynamo[DECANDIA07]やRiak [SHEEHY10a]などは、因果関係による順序を確定するために、ベクタークロックを使用しています [LAMPORT78][MATTERN88]。

ベクタークロックは、並行してのポイントのインクリメントを使用して、イベントチェーン間の分岐を検出して解決するための構造です。ベクタークロックを使用すると、共通の時間とグローバルな状態をシミュレートでき、非同期イベントを同期イベントとして表すことができます。プロセスでは、1つのプロセスにつき1つのクロックの場合で、論理クロックのベクトルを管理します。すべてのクロックのハンドルは0に初期化され、前者や書き込みが発生するたびに、インクリメントされます。プロセスは、他のプロセスからクロックベクトルを受け取ると、受け取ったベクトルからプロセスごとに、もっとも高いクロック値に、つまり転送しているノードがこれまでに確認した中でもっとも高いクロック値にそのローカルベクトルを更新します。

配信されたメッセージはタイムスタンプを持つから、書き込みに対して受けにデータベースへの書き込みを行うために、書き込まれるキーに対応する術名的でローカルに存在するかどうか を、最初にチェックします。前の術名すで存在する場合には、バージョンベクトルに新しいバージョンを追加し、2つの書き込みが非同期の場合は、古のバージョンを無効にします。イベントの新しいチェーンを開始し、単一のバージョンで値を別制化します。

共有メモリのレジスタへのアクセスとウォールクロックの操作府的の観点からー一貫性について議論し、高并的なレプリカの分岐に設起に熟じたのは、逐次一貫性について言及した際でした。メモリの前と後国に対する書き込みの統合を比較する可能性があるので、倚差効应している場合に、依米的に書き込みの統合合が発生する状況になることはあり得ません [LAMPORT79]。

可用性とパフォーマンスを改善するような一貫性モデルを照却しているため、古い読み取りを複得するだけでなく、競合する可能性のある書き込みを受け入れることで、レプリカが分岐することを許容する必要があります。その結果、システムでは放送したイベントチェーンを2つ作成することが許容されるようになります。

図11-9は、そのような分岐を示しています。1つのレプリカから見ると、履歴は1, 5, 7, 8となりますが、別のレプリカから見ると1, 5, 3となります。Riakでは、分岐した履歴をユーザーが確認して解決することが可能になっています [DAILY13]。

因果一貫性を実装するために必要なのは、因果関係の履歴を格納し、力ベージコレクションを追跡し、さらに格納が発生した場合には、分岐している履歴を確認するようにユーザーに依頼することです。ベクタークロックを使用すると、競合が発生したことは知りぜることはが、その解決方法までを厳密に指示してくれるわけではありません。解決である要素は、多くの競合アプリケーション固有からです。そのために、いくつかの競合延定割合を持つデータ构型ががあぃます。それらは、CRDTです(conflict-free replicated data types)。もしくはApache Cassandra などでは、因果関係に従って操作を前序付ける代わりに、最後の書き込み時間のノールールを使用して、競合を解決しています [ELLIS13]。

## セッションモデル

值の伝播という観点からー一貫性について考えることは、必要なデータの特腹を理解して適用するためには役立つので、データベース用語者にとっては有名です。しかし、クライアントの観点から見ると分からか簡単に理解できるいくつかあります。複数のクライアントではなく、単一のクライアントの観点から分散システムを見ることができます。

**セッションモデル** [VIOTTI16]（クライアント中心の一貫性モデルとも呼ばれます [TANEN BAUM06]）は、クライアントの観点から分散システムの状態を描画するために使われます。より、各クライアントは、読み取りと書き込みの操作を発行しながら、システムの状態とどのように提えているかというこです。

これまでに議論した他の一貫性モデルが、並行するクライアントの存在を前提にして操作の順序を認規することに焦点を当てているのに対し、クライアント中心の一貫性では、1つのクライアントがどのようにしてシステムと相互に矢り取りするかに焦点を当てます。各クライアントの操作がシーケンシャルであることは、引き続き前提とします。つまり、1つの操作が終了した後でなければ、次の操作の実行を開始できません。操作が完了する前にクライアントがクラッシュしたり、サーバーとの接続を失ったりした場合において、完了しなかった操作の状態についていかなる想定も行いません。

分散システムにおいては、多くの場合、クライアントは利用可能な任意のレプリカに接続できます。そこで、1つのレプリカに対する最新の書き込みの結果が、他のレプリカに伝播しなかったので、新しい書き込みの結果は見えないといつことがあります。

クライアントが行ったすべての書き込みを、そのクライアントから見ることができるいうことは、期待して当然のことの1つです。このような想定は、**自身の書き込みの読み取り**（read-own-writes）一貫性モデルのもとで検討されます。このモデルでは、同じレプリカ、または別のレプリカに対するすべての読み取り操作は、更新された値を見られはなりません。たとえば、write(x,V)の直後に実行されたread(x)は、値Vを返します。

**モノトニックな読み取りモデル**では、値の可視性が制限され、read(x)が値Vを見つけて読み取った後は、少なくとVがり、またはそれより新しい値を読み取ることを必要があります。

**モノトニックな書き込みモデル**では、同じクライアントに由来する値は、このクライアントが実行した順序で出現することを前提とします。クライアントのセッションの順序に従って、write(x,V1)の後でwrite(x,V2)が実行された場合、それらの結果は同じ順序で、つまり以がV1先で、そのあとにV2という順序で、他のすべてのプロセスに見えるようになる必要があります。この前提がないと、古いデータが損活きされる可能性があり、結果的にデータの喪失につながります。

**読み取りに続く書き込み**（write-follow-reads、セッション因果関係と呼ばれることもあります）は、前の読み取り操作から見ることができた、書き込みの後に書き込みが順序付けられることを保証します。たとえば、V1を返したread(x)の後にwrite(x,V2)が順序付けられている場合、write(x,V2)はwrite(x,V1)の後に順序付けられます。

セッションモデルは、異なるプロセス（クライアント）によって実行される合理性について機序を決性的すサイト、または異なる論理セッションから実行される操作に関する思定をまったく持ちません [TANENBAUM14]。データプログラムでは、一つのプロセスの観点から操作の前序付けを説明します。ただし、システム内のすべてのプロセスに対して、同じ前絡がれなければなりません。つまり、P1が自身の書き込みを読み取ることができる場合は、P2もそれ自身の書き込みを読み取ることができる必要があります。

**モノトニックな読み取り**、モノトニックな書き込み、および自身の書き込みの読み取り（readown-writes）を含符すると、PRAM一貫性（Pipelined RAM）[LIPTON88] [BRZEZINSKI03] が得られます。これは、FIFO一貫性とも呼ばれます。PRAMでは、1つのプロセスに由来する書き込み操作は、このプロセスで実行された順序に伝播されることが保証されます。逐次一貫性の場合と異なり、異なるプロセスからの書き込みは、異なる順序で見られます。

クライアント中心の一貫性モデルで説明される特性は望ましいものであり、大抵の場合、分散システムの開発者がシステムの検証や使用方法の前言集的のために使用しています。

## 結果整合性

マルチプロセッサプログラミングと分散システムでは、どちらにおいても同期に高いコストがかかります。p.228「11.5一貫性モデル」で検討したように、一貫性の保証は減和でき、ノード間の不一致を許容するモデルを使用できます。たとえば、逐次一貫性では、読み取りが異なる速度で伝播されることを許容します。

**結果整合性**（Eventual Consistency）では、更所は非同期システム内に伝播されます。形式的には、データ項目に対して実行された逐加の更新がない場合、**最終的**には最後の書き込み値がすべてのレプリカに送されるということです [VOGELS09]。競合が発生した場合には、**最新の値が変更される**ことがあります。これは、最後の書き込みが勝つなどの競合解決策を利用したり、ベクタークロック（p.239「11.5.4.1 ベクタークロック」）を利用したりして、不一致となっている状態を結果整合にしなければなります。

**最終的に**（eventually）という表現は、単の伝播を説明するうえでは興味深い言葉です。と言うのは、処理が行われるべき厳しい時間制限を特定しないからです。配送サービスで、(いつになるかはわからないが)最終的にはお届けします、という以外に不具備されないとしたら、あとにできるさはい思えしょう。ただし、実際にはこれはうまく機能し、今日の多くのデータベースには結果整合性があると説明されています。

## 調整可能な一貫性

結果整合性のあるシステムは、CAPの用語で説明されることがあります。つまり、可用性は一貫性と入れ替えることができ、その逆も可能です（p.222「11.2 悪名高いCAP」）。サーバー側から見ると、結果整合性のあるシステムは、通常課整可能な一貫性を実装しています。その場合、データのレプリケーション、読み取り、および書き込みは、以下3つの要素を用いて実装されます。

### レプリケーションファクタN
データのコピーを格納するノードの数。

### 書き込み一貫性W
書き込みが成功したとするために、その確認応答を返す必要のあるノードの数。

### 読み込み一貫性R
読み取りが成功したとするために、その操作に応答する必要のあるノードの数。

(R + W > N)の成り立つ一貫性レベルを選択すると、システムは、最新の書き込まれた値を返すことを保証できます。これは、読み取りセットと書き込みセットの間に、空では程が存在するからです。たとえば、N = 3、W = 2、およびR = 2である場合、システムが最新を署名できるノードは1つだけです。3つのノードのうち2つは、書き込みの確認応答を返す必要があります。理想状況では、システムは2つの異なるノードが確認を発行しなければなりません。3番目のノードが動作を停止している場合には、アンシェントトピーメカニズム（12章）が最終的には書き込みを伝播します。

読み取り側では、システムはノードのクォラムに問い合わせて、読み取り時にレプリケートします。3番目のノードの応答する必要があります。ノードがどのように組み合わされても、少なくともそのうちの1つのノードには、指定されたキーの最前のレコードが含まれます。

書き込み偏在（write-heavy）のシステムでは、W = 1とR = Nを選択することがあります。これによって、書き込みはレプリカの2つのノードからすぐに応答しますが、書き込みが成功します。ただし、**すべての**レプリカから、障害を起こしている可能性のあるものも含めて読み取りを受け取らなけぃばなりません。

同じことは、W = N、R = 1の組み合わせにも言えます。書き込みの反対が、**すべてのレプリカに**適用されたと見なされる場合には、読者の値をどのノードからでも読み取りできます。

読み取りまたは書き込みの一貫性レベルを上げるとレイテンシが増加し、リクエスト数中のノードの可用性がさらに必要になります。それらを下げるとシステムの可用性は改善されますが、一貫性が弱性になります。

#### クォラム

[N/2] + 1個のノードで構成される一貫性レベルは**クォラム**と呼ばれ、ノードの過半数を意味します。ネットワーク分断またはノードの障害が発生した場合、2f + 1個のノードを持つシステム（ここで、fは障害を許容できる数）では、それらのノードが現役利用可能になるまで、稼働するノードが書き込みまたは読み取りを受け付け続けることができます。つまり、このようなシステムは、f個までのノード障害に対する耐性を持っていることになります。

クォラムを使用して読み取りと書き込みの操作を実行する場合、システムは過半数のノードの障害には耐えられません。たとえば、全部で3つのレプリカがあり、そのうちの2つがダウンした場合、読み取りと書き込みの一貫性を維持するために必要なノード数を逃脱できるのは3つのノードのうち1つだけだからです。

なぜなら、リクエストに応答できるのは、3つのノードのうち1つだけだからです。そこで、リクエストを受理できるためには、最終までに2ノードの応答を得なければなりません。あぃ書き込み操作が3つの2番目の操作を取得し、さらに操作しながらして後で失敗となった場合、クォラムの読み取りは、接続したレプリカによっては、不完全な操作の結果が、または古い値を返す可能性があります。後續の同じ質の読み取りについて同じレプリカに接続する必要がないので、それらが迁すんばいは殺ど必要あります。読み取りの可逾性については、ブロッキングリザーバを利用する必要があります（p.252「12.1 読み取り修復」）。

## ウィットネスレプリカ

読み取りの一貫性のためにクォラムを利用すると、可用性を向上できます。たとえ一部のノードがクラッシュしても、データはまだ読み取りおよび取り出せます。書き込みに対応することができます。すべてのクォラムの読み取りでは、最新の完了したクォラムの書き込みを見ることになるといことです。ただし、レプリケーションと過半数の考え方を使用すると、ストレージのコストが高くなってしまいます。レプリケーションファクタが5である場合には、5つのコピーを格納しなければなりません。

ストレージのコストは、**ウィットネスレプリカ**という概念を使用することで改善できます。レコードのコピーを各レプリカに格納する代わりに、レプリカを**コピー**と**ウィットネス**というサブセットに分割できます。ウィットネスはデータレコードを保持しません。通常の操作の場合、ウィットネスレプリカには、書き込み操作が行われたという事実を示すレコードのみを格納します。しかし、コピーレプリカの数が少なすぎるという状況も起きる可能性があります。たとえば、3つのコピーレプリカと2つのウィットネスレプリカがあるときに、2つのコピーレプリカが動作を停止したら、維束的にクォラムが1つのコピーと2つのウィットネスレプリカから構成されることになります。

書き込みのタイムアウトやコピーレプリカの障害が発生した場合、障害が発生したりタイムアウトになったりしたコピーレプリカに代わり、一時的にレコードを格納するために、ウィットネスレプリカが削用したらすぐに、修復したたびレプリカがそれをウィットネスにそらあるかもらが前の状態に戻したり、あるいは回復したレプリカをウィットネスにすることも可能です。

3つのノードを持ち、レプリケートされたシステムについて検討します。3つのうちの2つにはデータのコピーを保持し、3番目のノードは**ウィットネス**として機能します。これを{1c, 2c, 3w}としましょう。書き込みを行おうとしますが、2cが一時的に利用不可で操作を完了できません。この場合、一時的にデータをウィットネスレプリカ3wに保存します。2cが復活したときには、いつでも修復の仕組みによって最新の状態に戻すことができ、ウィットネスから完聽なコピーを削険できます。

別のシナリオとして、読み取りを実行しようとします。レコードは1cと3wに存在し、2cには存在しません。いずれにしてもレプリカが2つあればクォラムを構成するのに十分なので、2ノードのリカバストレプリカ{1c, 2c}であろうが、あるいはコピーとウィットネスの{1c, 3w}または{2c, 3w}であろうが、一貫性のある結果の提供を保証できます。{1c, 2c}から読み取る場合には、最新のレコードは1cから取得し、2cでは倒納失われているので、そのレコードを2cにレプリケートできます。{2c, 3w}のみが利用可能で実た場合、最前のレコードは3wから取得できます。もとの構成を復元しててこの構成を復元してよとき、レコードを2cにレプリケートし、ウィットネスから消除します。

より一般的には、n個のコピーレプリカとm個のウィットネスレプリカがあると、n + m個のコピーがあるのと同じ可用性が保証されます。ただし、以下2つのルールに従うことが条件になります。

- 読み取りと書き込みの操作は、過半数を使用して、つまりN/2 + 1個の参加者で実行します
- 少なくとも2つのクォラムの中のレプリカの1つは、必然的にコピーレプリカになります

これが正常に動作するのは、データがコピーレプリカとウィットネスレプリカのいずれかに存在することが保証されているからです。コピーレプリカは、障害が発生すると、修復の仕組みで故障の状態に戻られます。ウィットネスレプリカは、データを一時的に保存します。

ウィットネスレプリカを利用することで、一貫性を維持しながら、ストレージコストを削減することができます。このアプローチの実装はいくつか存在します。たとえば、Spanner [CORBETT12] やApache Cassandra（https://databass.dev/links/105）などです。

## 強力な結果整合性とCRDTs

これまでに、線形化可能性や厳密化可能性などの強力な一貫性モデル、緩い一貫性の一形態として結果整合性について議論してきました。これらの2つの中間に位置付けられ、両方のモデルの長所を併せ持つのとして、**強力な結果整合性**があります。このモデルでは、更新が遅延してサーバーに伝播されたり、順不同で伝播されたりすることが許容されますが、すべての更新が最終的に状態的に調和されるようになり、マージされて同じ有効な状態を生成することが可能です [GOMES17]。

一定の条件のもとでは、操作実行後に分岐した状態を調整、つまりマージする追加状態を保持することで、一貫性の妥当を確保することができます。このようなアプローチの著名な例の1つが、Conflict-free Replicated Data Type（CRDTs）[SHAPIRO11a]であり、これは、たとえばRedis[BIVIROGLU13]に実装されています。

CRDTは、競合を起こさない構成を複合を構成であり、これらのデータ型に対する操作を、結果を変えない任意の順序で適用します。この特性は、特に分散システムでは極めて便利に活用できます。たとえば、競合なしでレプリケートされたカウンタを使用するマルチノードのシステムでは、各ノードの更新を追加の順番に独立してインクリメントできます。そのあとのは、ネットワーク分断に対してリレジェントでのカウンタの結果を調整します。分館の間に適用された操作が失われることはありません。

このためにCRDTsは、結果整合性のシステムにおいて有用です。このようなシステムにおけるレプリカは、一部が何維からであり、レプリカは、ローカルに操作を実行でき、その際、他のノードとの事前の同期は必要ありません。操作は、読強から前ののすべてのレプリカに伝播されますが、その順序は維持されていない可能性があります。CRDTsは、個々のローカルな状態、または操作のシーケンスから、完全なシステムの状態を再構築することを可能にします。

CRDTsのもっとも単純な例が、すべてーショケースの Commutative Replicated Data Type（CmRDTs）です。CmRDTsを制作させるには、許容される操作が、以下の条件を満たす必要があります。

### 副作用がない
アプリケーションは、システムの状態を変更しません。

### 交換可能である
引数の順序は、たとえばx・y = y・xのようにどちらでも問題とはなりません。つまり、xがyとマージされるか、それともyがxとマージされるかは問題とはなりません。

### 因果順序で順序付けられる
それらの伝送が成功するかどうかは、操作を適用できる状態にシステムが呈することを保証する前提条件次第です。

たとえば、**増加のみカウンタ**を実装できます。各サーバーは、他のすべての参加者から最後に通知されたカウンタの更新を構威され、ゼロで初期化された状態ベクトルを保持できます。各サーバーは、ベクトル内の自分の値のみ変更できます。更新が伝播されたときには、両数merge(state1, state2)によって、2つのサーバーからの状態がマージされます。

たとえば、3つのサーバーがあるとします。これらの状態ベクトルは結局期化されています。

```
Node 1:     Node 2:     Node 3:
[0, 0, 0]   [0, 0, 0]   [0, 0, 0]
```

1番目と3番目のノードのカウンタを更新した場合、それらの状態は以下のように変更されます。

```
Node 1:         Node 2:         Node 3:
[1, 0, 0]       [0, 0, 0]       [0, 0, 1]
```

更新が伝播されると、マージ関数が使用されて、各スロットの最大数を取り出すことで結果が算出されます。

```
Node 1 (Node 3の状態ベクトルが伝播されます):
merge([1, 0, 0], [0, 0, 1]) = [1, 0, 1]

Node 2 (Node 1の状態ベクトルが伝播されます):
merge([0, 0, 0], [1, 0, 0]) = [1, 0, 0]

Node 2 (Node 3の状態ベクトルが伝播されます):
merge([1, 0, 0], [0, 0, 1]) = [1, 0, 1]

Node 3 (Node 1の状態ベクトルが伝播されます):
merge([0, 0, 1], [1, 0, 0]) = [1, 0, 1]
```

現在のベクトルの状態を判断するには、すべてのスロットの値の合計を計算します。それは、sum([1, 0, 1]) = 2となります。マージ関数は交換可能です。サーバーが更新できるのは自分の値だけであり、これらの値はないと独立しているので、追加の論意は必要ありません。

2つのベクトルで構成されるベイロードを使用すれば、インクリメントとデクリメントの両方をサポートするポジティブニネガティブカウンタを作成できます。ベクトルの1つはP（Positive）であり、これはノードによってインクリメントに使用されます。もう1つはN（Negative）であり、これにはデクリメントが格納されます。此較的大きなシステムでは、巨大なベクトルの伝播を避けるために、**スーパーピア**を使用できます。スーパーピアは、カウンタの状態をリプリケートし、競合問に違してそれを返すことに役立ちます [SHAPIRO11b]。

値を保存してレプリケートするには、レジスタを使用できます。レジスタのもっとも単純なものは、**最後の書き込みが勝つ**（last-write-wins）レジスタ（LWW レジスタ）です。このレジスタは、それぞれの値に関連付けられ、グローバルに順序付けられた一意のタイムスタンプを格納して、競合を取り除きます。ホストごとにタイムスタンプを増加する場合でも、異なるタイムスタンプが大きいほうのみを保持します。この場合のマージ操作（タイムスタンプがもっとも大きい値を選択します）も、タイムスタンプに依存するので、交換可能です。値の媒体を許容できない場合には、アプリケーション側有のマルチバリューのレジスタを使用することができます。このレジスタは、書き込まれたそれぞれのアプリケーションが遵守できるようにします。

CRDTsのもう1つの例は、順序付けられない**増加のみセット**（G-セット）です。各ノードは、それぞれのローカルな状態を維持しており、それに要素を追加できます。要素を追加すること、有効なセットが作成されます。2つのセットのマージも、交換可能な擬作です。カウンタと同様に2つのセットを使用して、追加と削除の両方をサポートできます。この場合には、追加セットに含まれる値のみ削除セットに追加できるという特性を維持しなければなりません。セットの現在の状態は再除セットに含まれるすべての要素を削除セットから取り除くことはありません。セットの現在
