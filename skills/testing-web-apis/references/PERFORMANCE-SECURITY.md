# パフォーマンステスト・セキュリティテスト・本番環境テスト

## 1. パフォーマンステストの種類と目的

パフォーマンスは「非機能要件」と分類されがちだが、アプリケーションの全機能が正しく動作していても応答に時間がかかるなら、ユーザーにとって「機能していない」のと同じである。パフォーマンステストはテスト戦略の中で正当な優先順位を与えるべき品質特性である。

### 1.1 負荷テスト（Load Testing）

想定される通常の負荷水準でアプリケーションがどのように動作するかを確認する。目標の同時接続数（推計値や本番統計に基づく）をかけた状態で、可用性・スループット・レスポンスタイムが性能目標を満たしているかを評価する。

ユーザーの操作にかかる待ち時間（フォーム入力やページ遷移の間隔）も含めて、実際の利用パターンを忠実に再現することが重要である。

### 1.2 ストレステスト（Stress Testing）

アプリケーションを限界まで追い込み、限界点を特定する。仮想ユーザー数を段階的に増やし、エラー発生やレスポンスタイムの許容範囲超過が起きるポイントを見つける。

- アプリケーションの許容負荷の上限を把握できる
- パフォーマンス低下が始まるポイントと完全停止するポイントの両方を学べる
- DDoS攻撃への耐性の目安にもなる

### 1.3 スパイクテスト（Spike Testing）

急激な負荷の変動に対するシステムの耐性を検証する。短時間に大量のユーザーがアクセスした場合の挙動と、負荷が収まった後の回復能力を確認する。

セールイベント、メディア掲載、プッシュ通知配信などにより突発的なアクセス増加が予想されるシステムで有効である。

### 1.4 ソークテスト（Soak / Endurance Testing）

長時間にわたって一定の負荷を与え続け、時間経過に伴うパフォーマンス劣化を検出する。メモリリーク、ディスク容量の逼迫、コネクションプールの枯渇など、短時間のテストでは発見できない問題を炙り出す。

負荷テストやストレステストが1-2時間で終わるのに対し、ソークテストは数時間から数十時間にわたって実施する。レスポンスタイムやハードウェアリソース使用率の後半での急変（スパイク）がないかを確認する。

### 1.5 ベースラインテスト

単一の仮想ユーザーで指定回数だけ実行し、無負荷状態でのアプリケーション性能を計測する。この結果と負荷テスト・ストレステストの結果を比較すると、負荷増加時の性能劣化の度合いが定量的に把握できる。

---

## 2. パフォーマンステストの計画と実施

### 2.1 テスト環境の準備

パフォーマンステストの結果の信頼性は、テスト環境が本番環境にどれだけ近いかに依存する。

**環境構築の要点**:
- **インフラ構成** --- ロードバランサ、コンテナ数、データベースレプリケーション等を本番と合わせる
- **データベースのデータ量** --- 空のデータベースでテストしない。本番相当のデータ量をロードする
- **キャッシュのウォームアップ** --- 本番同様にキャッシュを温めてからテストを開始する
- **外部サービス** --- 依存する外部APIの応答時間も考慮に入れる

### 2.2 テストシナリオの設計: ユーザーフロー

パフォーマンステストスクリプトは、合理的に可能な限り本番環境でのユーザーの実際のふるまいを再現すべきである。

**ユーザーフローの構成要素**:
- **フローの説明と負荷設定** --- 仮想ユーザー数、ランプアッププロファイル（負荷の増加パターン）
- **ステップの詳細** --- ユーザーが取る行動の順序と各ステップで送信するAPIリクエスト
- **テストデータ** --- 各ステップで必要なデータセットの定義
- **待ち時間** --- ユーザーの操作間隔を再現するためのランダム化した遅延（例: 5000ms +/- 2500ms）

単一エンドポイントへの大量リクエストではなく、実際のユーザー操作フローをエミュレートすることで、現実的な結果を得られる。

### 2.3 パフォーマンス指標

| 指標 | 説明 | 確認方法 |
|------|------|---------|
| レイテンシ | リクエスト送信からレスポンス受信までの時間 | 平均値、中央値、パーセンタイル（p95, p99） |
| スループット | 単位時間あたりの処理リクエスト数 | リクエスト/秒（RPS） |
| エラー率 | 全リクエスト中のエラーレスポンスの割合 | 4xx/5xxステータスの集計 |
| 可用性 | 負荷下でサービスが応答を返せた割合 | 成功レスポンス率 |
| リソース使用率 | CPU、メモリ、ネットワークI/O、ディスクI/Oの使用状況 | サーバー監視ツール |

**KPI（重要性能指標）**:
- **低水準KPI** --- CPU使用率、メモリ使用率、ディスクI/O、ネットワーク帯域幅
- **サーバーKPI** --- Webサーバー（Spring Boot、Express等）が提供する固有の指標
- **データベースKPI** --- データベースの接続数、ロック状況、クエリ実行時間

### 2.4 性能目標の設定

パフォーマンステストには必ず明確な目標を設定する。目標がなければ結果を評価できない。

**目標の設定例**:
- 「同時接続5,000ユーザーで可用性99%以上」--- ストレステストの目標
- 「200ユーザーが24時間アクセスし続けて平均レスポンスタイム1,000ms以下」--- ソークテストの目標
- 「同時接続2,000ユーザーでネットワーク使用率50%未満」--- 負荷テストの目標

目標はユーザーの品質特性（安定性、可用性など）から導出する。品質特性 → 計測指標 → 達成すべき数値という順序で設定する。

### 2.5 ツールの概念

パフォーマンステストツールは、仮想ユーザーを生成してAPIにリクエストを送り、レスポンスの指標を収集する。代表的なツールには以下のカテゴリがある。

- **スクリプトベース** --- k6（JavaScript）、Gatling（Scala DSL）、Locust（Python）など。コードでテストシナリオを記述する
- **GUI + スクリプト** --- JMeter（XML/GUI）など。GUIでシナリオを構築し、CLIで実行する
- **分散実行** --- 負荷生成を複数のワーカーノードに分散し、テストハーネス自体のリソース不足を回避する

### 2.6 結果の分析

パフォーマンステストで最も難しいのは結果の分析である。指標が示すのはイシュー自体ではなく、イシューの兆候であることが多い。

**分析の観点**:
- **レスポンスタイム x ユーザー数** --- 接続ユーザー数の増加に伴うレスポンスタイムの推移を追跡する
- **スループットの推移** --- ある時点でスループットが頭打ちになったり低下したりしていないかを確認する
- **エラーの出現タイミング** --- 4xx/5xxエラーが現れ始めた時点のユーザー数やリソース使用率を確認する
- **KPIとの相関** --- パフォーマンスツールの指標とサーバー/DBのKPIを突き合わせ、ボトルネックを特定する

---

## 3. セキュリティテストの基礎

### 3.1 セキュリティテストのマインドセット

セキュリティテストは特別な超人的スキルを必要とする専門家だけの仕事ではない。セキュリティテストの本質は、品質に影響を与えるリスクの発見と緩和である。セキュリティという品質特性にフォーカスしているだけで、モデリング・リスク分析・探索テスト・自動化といった通常のテスト技法がそのまま適用できる。

### 3.2 OWASP API Security Top 10

OWASPが公開するAPIセキュリティの代表的な脅威リストは、セキュリティテストのチェックリストとして活用できる。

| 脅威カテゴリ | 概要 |
|-------------|------|
| 認証の不備 | APIキーやトークンの管理不備、弱いパスワードポリシー |
| 認可の不備 | オブジェクトレベル・機能レベルの権限チェック漏れ |
| 過剰なデータ公開 | 必要以上の情報をレスポンスに含めてしまう |
| リソース制限の欠如 | レート制限やページネーション未設定 |
| インジェクション | SQL、NoSQL、コマンドインジェクション |
| セキュリティ設定ミス | デバッグエンドポイントの公開、不要なHTTPメソッドの許可 |

このリストはAPI固有のセキュリティ観点を網羅しており、脅威モデリングの補完として活用すると、見落としを防げる。

### 3.3 認証・認可テスト

**認証テスト**:
- デフォルトパスワードや弱いパスワードでのログイン試行
- ブルートフォース攻撃への耐性（ロックアウト機構の有無）
- トークン（JWT、OAuth）の有効期限と失効処理の検証
- セッション管理（セッション固定攻撃、セッションハイジャック対策）

**認可テスト**:
- 水平権限昇格 --- 他ユーザーのリソースにアクセスできないか（例: `/users/123` のIDを変更して `/users/456` にアクセス）
- 垂直権限昇格 --- 一般ユーザーが管理者APIにアクセスできないか
- APIキーのスコープ --- 読み取り専用キーで書き込み操作ができないか

### 3.4 インジェクション攻撃の検証

APIへの入力値を通じた攻撃パターンを検証する。

- **SQLインジェクション** --- クエリパラメータやリクエストボディにSQL文を注入し、データベースを不正操作する
- **NoSQLインジェクション** --- MongoDBの `$gt`、`$ne` 等のオペレータを注入する
- **コマンドインジェクション** --- OSコマンドを注入してサーバー上で任意コマンドを実行する

対策の基本は「すべての外部入力をバリデーションする」「プリペアドステートメントを使用する」「入力値をエスケープする」の3点である。テストでは、これらの対策が正しく機能しているかを検証する。

### 3.5 入力バリデーションの検証

- 期待するデータ型と異なる型（文字列フィールドに数値、整数フィールドに浮動小数点数）
- 境界値（0、負数、最大値 +1）
- 特殊文字（`<script>`、`' OR 1=1 --`、null文字）
- 過大なペイロード（巨大なJSONボディ、長大な文字列）
- 必須フィールドの欠落

### 3.6 レート制限のテスト

APIのレート制限が正しく機能しているかを検証する。

- 制限値を超えるリクエストを送信し、429（Too Many Requests）が返されることを確認
- レート制限のリセット期間が経過した後、リクエストが再び受け入れられることを確認
- 異なるAPIキーやIPアドレスでの制限が独立して動作することを確認

---

## 4. 脅威モデリング

### 4.1 データフロー図（DFD）によるモデル作成

脅威モデリングの第一歩は、システムのデータの流れを可視化することである。DFDは以下の要素でシステムを表現する。

- **外部エンティティ**（四角形）--- システムとやり取りするユーザーや外部サービス
- **プロセス**（円形）--- データを処理するコンポーネント
- **データストア**（平行線）--- データが格納される場所
- **データフロー**（矢印）--- データの移動経路
- **信頼境界**（菱形）--- 信頼レベルが変わる境界

DFDによりデータの流れが明確になり、保護すべき資産（機密データ、認証トークン等）と攻撃されうるポイントが視覚的に把握できる。

### 4.2 STRIDE分析

STRIDEは、DFDモデルの各要素に対して6つの脅威カテゴリを体系的に検討するフレームワークである。

| 頭文字 | 脅威 | 概要 |
|--------|------|------|
| **S** | Spoofing（なりすまし） | 偽の情報で他人のふりをしてシステムにアクセスする |
| **T** | Tampering（改竄） | 通信中や保存中のデータを不正に書き換える |
| **R** | Repudiation（否認） | 不正行為の痕跡を消し、行為を否定可能にする |
| **I** | Information Disclosure（情報漏洩） | 権限のない者に機密情報が漏洩する |
| **D** | Denial of Service（サービス妨害） | システムを利用不能にする攻撃 |
| **E** | Elevation of Privilege（権限昇格） | 本来持たない特権を不正に獲得する |

DFDモデルの各要素に対してSTRIDEの6カテゴリを順番に検討し、該当する脅威をモデルに書き加える。複数の脅威カテゴリが組み合わさる攻撃（例: なりすまし + 情報漏洩）も意識する。

### 4.3 脅威ツリーによる深掘り

STRIDEで特定した脅威を、脅威ツリー（攻撃ツリー）を使ってさらに具体的な攻撃手法に分解する。

```
なりすまし（Spoofing）
├── 推測による攻撃
│   ├── ブルートフォース（パスワード総当たり）
│   └── デフォルトパスワードの利用
├── 認証情報の盗取
│   ├── トークンの漏洩（ログ、公開リポジトリ）
│   └── 中間者攻撃（MITM）
└── ソーシャルエンジニアリング
    ├── フィッシング
    └── なりすましメール
```

### 4.4 優先順位付け: DREADモデル

脅威の対処優先順位は、DREADモデルで定量的に評価できる。

| 項目 | 評価基準（1-10） |
|------|----------------|
| **D**amage（被害） | 攻撃が成功した場合のダメージの大きさ |
| **R**eproducibility（再現性） | 攻撃をどれだけ容易に再現できるか |
| **E**xploitability（悪用容易度） | 攻撃にどれだけの技術スキルが必要か |
| **A**ffected users（影響範囲） | 影響を受けるユーザーの割合 |
| **D**iscoverability（発見容易度） | 攻撃者がこの脆弱性をどれだけ容易に発見できるか |

合計スコア / 5 = 優先度スコア。10に近いものから対処する。

---

## 5. 本番環境でのテスト戦略

### 5.1 カナリアリリース

新バージョンのデプロイ時に、全トラフィックを一度に切り替えるのではなく、一部のユーザー（例: 5%）にだけ新バージョンを配信する手法。新バージョンのエラー率やレスポンスタイムを旧バージョンと比較し、問題がなければ段階的に配信割合を拡大する。

**確認ポイント**:
- エラー率が旧バージョンと同等以下か
- レスポンスタイムの劣化がないか
- ビジネスメトリクス（コンバージョン率等）への悪影響がないか

問題を検出した場合は即座にロールバックできるため、リリースのリスクを大幅に低減できる。

### 5.2 フィーチャーフラグ

コード内に条件分岐を埋め込み、特定の機能をデプロイ後に動的にオン/オフ切り替えできる仕組み。

**テストへの活用**:
- 本番環境で新機能を限定的に有効化し、実データでの挙動を観察
- 問題発生時にコードデプロイなしで即座に機能を無効化
- A/Bテストの基盤として、ユーザーグループごとに異なる機能を提供

### 5.3 合成モニタリング（Synthetic Monitoring）

実ユーザーのトラフィックとは別に、テストスクリプトを定期的に実行してシステムの可用性とパフォーマンスを継続的に監視する手法。

- **定期的なヘルスチェック** --- 主要なAPIエンドポイントへの定期リクエストでステータスコードとレスポンスタイムを監視
- **重要なユーザーフローの再現** --- ログイン → 操作 → ログアウトといった一連のフローを自動実行
- **リージョン別の監視** --- 複数の地理的拠点からリクエストを送り、地域ごとのパフォーマンスを把握

合成モニタリングにより、実ユーザーが影響を受ける前に問題を検出できる。

### 5.4 オブザーバビリティとの連携

パフォーマンステストとセキュリティテストの知見を、本番環境のオブザーバビリティ基盤に反映する。

**ログ**:
- パフォーマンステストで特定したボトルネック箇所に詳細ログを追加
- 認証失敗やレート制限超過のログを構造化して記録

**メトリクス**:
- パフォーマンステストのKPIと同じ指標を本番環境で継続的に収集
- アラート閾値をパフォーマンステストの性能目標に基づいて設定

**トレース**:
- 分散トレーシングでAPI間のリクエストフローを追跡
- コントラクトテストで検出した依存関係に対応するトレースを監視

### 5.5 反復的なアプローチ

パフォーマンステストもセキュリティテストも、一度実施して終わりではない。

- **パフォーマンステスト** --- 新機能追加やアーキテクチャ変更のたびにユーザーフローを更新し、テストを再実行する。ベースラインとの比較で回帰を検出する
- **セキュリティテスト** --- 脅威モデルを定期的に見直し、新たな脅威（使用ライブラリの脆弱性、新たな攻撃手法）に対応する。依存コードチェックはCI/CDで継続的に実行する
- **本番モニタリング** --- テスト結果をモニタリング設定にフィードバックし、検出精度を継続的に向上させる
