# データローカリティとメモリー活用のトレードオフ

## 概要

ビッグデータ処理の本質的なトレードオフは「データを計算処理に運ぶ」か「計算処理をデータに運ぶ」かの選択だ。小規模データなら前者で十分だが、テラバイト・ペタバイト規模では実現不可能になる。

**データローカリティ**は計算ロジックをシリアライズしてデータノードへ送信し、ネットワーク転送量を「全データ」から「ロジックのバイトコード + 結果のみ」に削減する手法だ。これにより処理のスケールアウトが線形に実現できるが、ロジックのシリアライズ・デシリアライズの複雑さが伴う。I/Oバウンドな処理をCPUバウンドに転換することで大幅なパフォーマンス向上を得られる。

## トレードオフ分析

### データ処理アーキテクチャの選択

| アーキテクチャ | ボトルネック | スケール方法 | 適用限界 |
|-------------|-----------|------------|---------|
| データを計算に移動 | ネットワーク転送 | ネットワーク帯域増加（困難） | データがRAMに収まらなくなると不可能 |
| 計算をデータに移動（データローカリティ） | CPU処理 | データノードのコア数増加（容易） | ロジックのシリアライズ実装が必要 |

### パーティション化アルゴリズムの選択

| アルゴリズム | 読み取り最適化 | 書き込み最適化 | スケール変更コスト | 適用場面 |
|------------|------------|------------|--------------|---------|
| レンジパーティション（日付ベース） | 時系列クエリー | 均等分散しやすい | 低（新規パーティション追加のみ） | オフライン・コールドデータ |
| ハッシュパーティション（IDベース） | ID指定クエリー | 均等分散 | 高（ノード追加時に大量再配置） | オンライン・特定エンティティアクセス |
| コンシステントハッシュ | ID指定クエリー | 均等分散 | 低（仮想スロットのみ再配置） | ノード追加・削除が頻繁なシステム |

> **ハッシュパーティションの落とし穴**: ノード数Mで剰余計算をすると、ノード追加時に約70%のデータを再配置する必要が生じる。本番システムではコンシステントハッシュアルゴリズムを採用すること。

### パーティション粒度の選択

| 粒度 | メリット | デメリット | 適用基準 |
|-----|---------|---------|---------|
| 年単位 | ファイル数が少ない | 特定日のクエリーに全年データスキャンが必要 | データ量が少ない場合のみ |
| 年/月/日単位（推奨） | 柔軟なクエリー（日・月・年すべて対応） | ファイル数が多い | 100億レコード以上の標準 |
| 時間・分・秒単位 | 1日分が1ノードに収まらない場合 | 管理が複雑 | 1日のデータが1ノードに収まらない時 |

### 結合戦略の選択

| 結合戦略 | データ移動 | 前提条件 | 処理速度 |
|---------|---------|---------|---------|
| 狭い変換（Narrow Transformation） | なし | 同じ物理ノードに結合対象データが揃っている | 最速 |
| 広い変換 + データシャッフル | あり | 異なるパーティションにまたがるデータ | 低速（シャッフル量に比例） |
| ブロードキャスト結合 | 小さいデータセットのみ転送 | 片方のデータセットがRAMに収まる | 中間（最も実用的） |

### ストレージ選択のコスト比較（100GBデータ処理の実測値）

| ストレージ | 処理時間 | コスト（相対） | 適用場面 |
|---------|---------|------------|---------|
| RAM（メモリー内処理） | 約25秒 | 高 | 中間結果・頻繁にアクセスするデータ |
| SSD | 約100秒（RAM比4倍） | 中 | 高速応答が必要なコールドデータ |
| HDD | 約33分（RAM比80倍） | 低 | 大量・低頻度アクセスのアーカイブデータ |

**RAMベース処理の優位性**: ディスクベース（Hadoop方式）は各ステージの中間結果をディスクに書き出す。RAMベース（Spark方式）は最初のロードと最終書き出しのみディスクアクセス。10ステージのパイプラインでは差が10倍以上に拡大する。

## よくある誤り

### 1. データシャッフルを考慮しない結合設計

異なる日付パーティションにまたがるユニークユーザー抽出など、広い変換が必要な処理を設計する際にシャッフルコストを見落とす。全パーティションのデータがネットワークを流れることで処理が極端に遅くなる。

**対策**: 結合前に「同じ物理ノードにデータを集められるか」を確認する。日付ベースのパーティション設計なら、同日のデータは1ノードにまとめることでデータローカリティを確保できる。

### 2. ブロードキャスト対象データの無秩序な成長

ブロードキャスト結合では小さいデータセットを全ノードのRAMに保持する。「今は小さいから大丈夫」と思って設計し、データ成長後にメモリー不足で処理失敗するケース。

**対策**: ブロードキャスト対象データのサイズ上限を事前に定義し、監視する。RAMに収まらなくなったら戦略を変更する設計にしておく。

### 3. ホットパーティション（データ偏り）の見落とし

特定のIDや日付にデータが集中すると、そのパーティションを担当するノードだけがボトルネックになる。均等なハッシュ分散でも、実際のアクセスパターンが偏っていると意味がない。

**対策**: パーティション設計時にデータ分布を確認する。1つのパーティションキーが全体の20%以上を占める場合はサブパーティション化を検討する。

### 4. コールドデータに必要以上に細かいパーティション

将来の読み取りパターンが不明なコールドデータに対して、書き込み最適化のパーティション（ユーザーIDなど）を採用する。後から日付範囲クエリーが必要になった時に全パーティションスキャンが必要になる。

**対策**: アクセスパターンが不明なコールドデータは日付ベースのパーティションを採用する。

### 5. 手動最適化が標準オプティマイザーを下回るケース

ブロードキャストヒントを手動で追加した結果、実行エンジンの標準クエリーオプティマイザーが選択する戦略よりも遅くなることがある。

**対策**: カスタム最適化を加える前後で必ず計測・比較する。標準オプティマイザーが選択した実行計画（物理プラン）を確認し、意図通りの最適化が行われているかを検証する。

## 実践ガイドライン

### パーティション設計の意思決定フロー

```
データ特性を確認
    ├── オフライン・コールドデータ?
    │   └── 日付ベース（年/月/日）パーティション
    │       └── 1日のデータが1ノードに収まる? → 完了
    │           └── No → 時間・分・秒でさらに細分化
    │
    └── オンライン・頻繁アクセス?
        ├── 読み取りパターンが既知?
        │   └── クエリーキーでハッシュパーティション
        │       └── ノード増減が頻繁? → コンシステントハッシュ
        └── 読み取りパターンが不明
            └── 日付ベースパーティション + クエリーキーのインデックス
```

### 結合戦略の選択基準

```
2つのデータセットを結合する場合
    ├── 同じ物理ノードにある? → 狭い変換（追加コスト0）
    │
    ├── 片方のデータセットが全ノードのRAMに収まる?
    │   → ブロードキャスト結合（推奨）
    │   注意: ブロードキャスト対象のサイズを事前に確認
    │
    └── 両方が大きい?
        → 広い変換 + データシャッフル
        最適化: パーティションキーを結合キーに一致させる
```

### MapReduceパイプライン設計のポイント

MapReduceは3つのフェーズで構成される：

1. **Mapフェーズ**: データローカリティを活用してローカルデータを処理し、`(パーティションキー, 値)` ペアを生成
2. **シャッフルフェーズ**: 同じパーティションキーを持つデータを同じノードに集約（ここがボトルネック）
3. **Reduceフェーズ**: 集約されたデータをローカルで処理（再度データローカリティを活用）

シャッフル量を減らすには：
- Mapフェーズでローカル集約（Local Reduce）を先に実施する
- パーティションキーのカーディナリティ（種類数）を適切に設定する
- データ偏りを事前に把握し、ホットキーの分散処理を設計する

## まとめ

- **計算をデータに運ぶ**: ネットワーク転送量を最小化し、I/OバウンドをCPUバウンドに転換することで大幅なパフォーマンス向上を得られる
- **パーティション粒度の選択**: コールドデータには日付ベース、オンラインデータはアクセスパターン最適化。粒度が荒すぎると読み取りが非効率、細かすぎると管理が複雑
- **ブロードキャスト結合の前提条件**: 小さいデータセットが全ノードのRAMに収まる場合のみ適用。データ成長を監視する設計が必要
- **RAMベース処理の優位性**: ディスクベース（Hadoop）はRAMベース（Spark方式）と比べて処理速度が数倍〜80倍劣る。中間ステージ数が多いほど差が拡大
- **コンシステントハッシュ**: ノード追加・削除が頻繁なシステムでは素朴なハッシュ剰余計算を避け、コンシステントハッシュを採用する
- **計測ファースト**: カスタム最適化は標準オプティマイザーを下回ることがある。変更前後で必ず計測・比較する
