# 監視運用：アラート、オンコール、インシデント管理

本ドキュメントは designing-monitoring スキルの一部です。アラート設計、オンコール運用、インシデント管理、振り返り、および監視のための統計知識を提供します。

---

## 1. アラート設計ベストプラクティス

### 1.1 アラートの分類と通知方法

アラートは以下の2つに分類し、異なる通知方法を使用します:

| 分類 | 対応要件 | 通知方法 | 例 |
|-----|---------|---------|-----|
| **緊急対応アラート** | 即座に人間の介入が必要 | SMS、電話、PagerDuty等のページング | サービス全停止、データ損失の危険 |
| **FYI通知** | 情報共有、後で確認すれば良い | Slack、Teams等のチャットツール | 軽微な性能劣化、容量警告 |

**重要**: アラートにメールを使わないこと。メールは読まれず、緊急性が伝わりません。

### 1.2 手順書 (Runbook) の必須化

すべての緊急対応アラートには手順書を紐付けます。手順書には以下を含めます:

- **サービス概要**: 何のサービスか
- **影響範囲**: ユーザーへの影響
- **初期対応手順**: 最初に何をすべきか（具体的なコマンド含む）
- **エスカレーション条件**: どの時点で上位者に連絡するか
- **関連リソース**: ダッシュボードURL、ログ検索クエリ、関連ドキュメント

手順書テンプレートは本ドキュメント末尾を参照してください。

### 1.3 固定閾値以外のアプローチ

単純な固定閾値（例: CPU > 80%）だけでなく、以下のアプローチを検討します:

| 手法 | 概要 | 適用例 |
|-----|------|--------|
| **Moving Average** | 過去N分の移動平均を閾値とする | 時系列データの短期的なスパイクを無視 |
| **Confidence Band** | 正常範囲の上限・下限を統計的に決定 | 周期性のあるメトリクス（日次/週次パターン） |
| **Standard Deviation** | 平均 ± N × 標準偏差を閾値とする | 正規分布に従うメトリクス |

詳細は「5. 監視のための統計入門」を参照してください。

### 1.4 アラートの定期チューニング

月次でアラートをレビューし、以下の3つの質問を各アラートに問います:

1. **即時対応が必要か？** → 必要ない場合はFYI通知に変更
2. **自動復旧できないか？** → 可能ならauto-healing実装
3. **このアラートは価値があるか？** → なければ削除

**誤報が続くアラート**は即座にチューニングまたは削除します。「アラート疲れ」は本当の問題を見逃す原因になります。

### 1.5 メンテナンス期間の活用

計画停止時はアラートを抑制します:

- 定期メンテナンス用のダウンタイムスケジュール設定
- メンテナンスウィンドウ中のアラート自動抑制
- 事前にステークホルダーへの通知

### 1.6 自動復旧 (Auto-Healing)

人間の介入なしで復旧可能な問題は自動化します:

**フロー**:
1. アラート発報
2. 自動復旧スクリプト実行（例: サービス再起動、キャッシュクリア）
3. 復旧確認
4. 成功 → FYI通知のみ送信
5. 失敗 → 緊急対応アラート発報

**注意**: 自動復旧が実行された場合も必ずログに記録し、根本原因の調査を後日実施します。

---

## 2. オンコール運用

### 2.1 誤報率の管理

**誤報 (False Alarm)** とは、アラートが発報されたが実際には対応不要だったケース。

- 誤報率を追跡（例: 週次集計）
- 100%正確なアラートは不可能だが、継続的改善が必須
- 目標: 誤報率 < 10%

### 2.2 場当たり的対応の削減

アラート対応時に以下の2つの質問を自問します:

1. **この対応を自動化できないか？** → 自動復旧スクリプトの実装を検討
2. **根本原因を修正できないか？** → 一時しのぎではなく恒久対応を計画

### 2.3 ローテーション設計の原則

#### 最小チーム規模

- **最小4人チームが必要**
- 理由: 1人7日ローテでも休暇・病欠をカバーするには4人必要
- 3人以下のチームは持続不可能

#### オンコール期間

- **1回のオンコール期間は最長1週間**
- 長すぎるとバーンアウトのリスク
- 短すぎると引き継ぎコストが増大

#### Follow-the-Sun (FTS) ローテーション

タイムゾーンを跨いだローテーション。各拠点が日中のみ対応します。

| メリット | 注意点 |
|---------|--------|
| 夜間対応が不要 | 引き継ぎのオーバーラップが必須（1-2時間） |
| ワークライフバランス向上 | 各拠点の最小人数確保（各拠点2人以上推奨） |
| グローバル展開に適合 | コミュニケーションツールの整備が必要 |

### 2.4 オンコール補償

オンコール担当者には以下のいずれかの補償を提供します:

- **金銭的補償**: オンコール手当、対応時の時間外手当
- **代休**: オンコール期間後の休暇付与

補償なしのオンコールは離職率を高めます。

---

## 3. インシデント管理

### 3.1 インシデント対応フロー

ITIL準拠の標準フロー:

```
1. 検知 (Detection)
   ↓
2. 対応開始 (Response)
   ↓
3. 修復 (Mitigation) ← **サービス復旧を最優先**
   ↓
4. 根本原因調査 (Root Cause Analysis)
   ↓
5. 解決 (Resolution)
   ↓
6. 事後レビュー (Post-Incident Review)
```

**重要**: ステップ3で「サービス復旧」を最優先とし、根本原因調査は後回しにします。ユーザーへの影響を最小化することが第一目標です。

### 3.2 インシデントコマンダー (IC) の役割

大規模インシデントでは **Incident Commander (IC)** を指名します:

| 役割 | 説明 |
|-----|------|
| **コミュニケーションの統制** | 全メンバーへの指示、ステータス更新の統括 |
| **タスクの委譲** | 誰が何をするかを明確化 |
| **外部ステークホルダーへの報告** | 経営層、顧客、営業チームへの状況共有 |
| **意思決定** | トレードオフ判断（例: サービス一時停止 vs 部分復旧） |

ICは技術的作業を行わず、オーケストレーションに専念します。

### 3.3 コミュニケーション体制

インシデント発生時のコミュニケーションルール:

- **専用チャンネル/ブリッジの設置**: インシデント専用のSlackチャンネルやZoom会議室
- **定期的なステータス更新**: 30分ごと（大規模の場合は15分ごと）
- **顧客向け通信の管理**: ステータスページの更新、サポートチームへの情報提供

**テンプレート例**:
```
【ステータス更新 HH:MM】
- 現状: [簡潔な状況説明]
- 影響範囲: [影響を受けているユーザー/機能]
- 次のアクション: [次に何をするか]
- 次回更新: [HH:MM予定]
```

---

## 4. 振り返り (Post-Incident Review)

### 4.1 ブレームレス文化

振り返りの大原則: **「誰が悪いか」ではなく「何が悪かったか」**

- 個人を責めない
- システムやプロセスの改善に焦点を当てる
- 失敗から学ぶ文化を醸成

### 4.2 振り返りの構造

インシデント後48時間以内に振り返りを実施し、以下を文書化します:

#### タイムライン

時系列で何が起こったかを記録:

```
13:45 - アラート発報（CPU使用率 > 95%）
13:47 - オンコール担当者が確認開始
13:52 - データベース接続数が上限到達と判明
14:05 - 接続プール設定を拡大
14:12 - サービス復旧確認
```

#### 影響範囲

- 影響を受けたユーザー数
- 機能停止の範囲
- ダウンタイム（分単位）
- 金銭的損失（推定）

#### 根本原因

5 Whys手法等を用いて根本原因を特定:

```
問題: データベース接続数が上限到達
Why 1: なぜ接続数が急増したか？ → キャンペーン開始でトラフィック増加
Why 2: なぜ接続プールが対応できなかったか？ → 接続プール設定が小さすぎた
Why 3: なぜ設定が小さいままだったか？ → トラフィック予測が甘かった
Why 4: なぜ予測が甘かったか？ → 過去のキャンペーンデータを分析していなかった
Why 5: なぜ分析していなかったか？ → キャパシティプランニングプロセスが未整備
```

#### アクションアイテム

具体的な改善策を担当者・期限付きでリストアップ:

```
- [担当: Aさん、期限: 2週間] キャパシティプランニングプロセスの文書化
- [担当: Bさん、期限: 1週間] 接続プール監視アラートの追加
- [担当: Cさん、期限: 1ヶ月] 過去キャンペーンデータの分析ダッシュボード作成
```

### 4.3 振り返りの公開

振り返りドキュメントは組織全体に公開します:

- 透明性の向上
- 他チームが同様の問題を予防できる
- 学習する組織文化の構築

---

## 5. 監視のための統計入門

動的閾値の設計やメトリクス分析に必要な統計知識を解説します。

### 5.1 Mean vs Median の違いと使い分け

| 統計量 | 説明 | 適用例 |
|-------|------|--------|
| **Mean (平均)** | 全データの合計 ÷ データ数 | 外れ値の影響を受けやすい。正規分布データに適用 |
| **Median (中央値)** | データを並べた際の中央の値 | 外れ値の影響を受けにくい。レイテンシ測定に推奨 |

**使い分け**:
- **レイテンシ/レスポンスタイム**: Median（中央値）またはパーセンタイル（p95, p99）を使用
- **CPU使用率**: Mean（平均）でも可だが、Medianの方が安定
- **リクエスト数**: Sumが基本だが、平均化する場合はMean

### 5.2 周期性 (Periodicity)

多くのメトリクスには周期的なパターンがあります:

- **日次パターン**: 日中は高負荷、夜間は低負荷
- **週次パターン**: 平日は高負荷、週末は低負荷

**アラート設計への応用**:
- 固定閾値ではなく、曜日・時間帯ごとに異なる閾値を設定
- 例: 平日9-18時は CPU > 80%、それ以外は CPU > 60% でアラート

### 5.3 分位数/パーセンタイル

**定義**: データを並べた際の特定位置の値

| 分位数 | 意味 |
|--------|------|
| **p50 (50パーセンタイル)** | 中央値。50%のリクエストがこの値以下 |
| **p95 (95パーセンタイル)** | 95%のリクエストがこの値以下。上位5%を除外 |
| **p99 (99パーセンタイル)** | 99%のリクエストがこの値以下。上位1%を除外 |

**レイテンシモニタリングの標準**:
- p50: 典型的なユーザー体験
- p95: ほとんどのユーザーの体験
- p99: 最悪ケースに近いユーザー体験

**推奨**: SLOはp95またはp99で設定します（例: p95レイテンシ < 200ms）

### 5.4 標準偏差 (Standard Deviation)

**定義**: データのばらつきを表す指標

- 標準偏差が小さい → データが平均の周りに集中
- 標準偏差が大きい → データが広く分散

**正常範囲の定義**:
- **平均 ± 1 × 標準偏差**: 約68%のデータが含まれる
- **平均 ± 2 × 標準偏差**: 約95%のデータが含まれる
- **平均 ± 3 × 標準偏差**: 約99.7%のデータが含まれる

**異常検知への応用**:
- 平均 + 3 × 標準偏差を超えた値を異常とみなす
- ただし、データが正規分布に従う場合にのみ有効

### 5.5 集約の注意点

**重要**: 「集約の集約」は統計的に無効です。

**❌ 誤った例**:
- サーバA、B、Cの平均CPU使用率をそれぞれ計算
- 3つの平均をさらに平均化する

**✅ 正しい例**:
- すべてのサーバの生データを集約してから平均を計算
- または、各サーバごとに個別に監視

**理由**: 各サーバのデータ数が異なる場合、二重集約は重み付けが不正確になります。

---

## 6. アラート手順書テンプレート

すべての緊急対応アラートには以下の構造の手順書を作成します。

### サービス名: [サービス名]

#### メタデータ

- **リポジトリ**: [GitHubリポジトリURL]
- **ドキュメント**: [技術ドキュメントURL]
- **ダッシュボード**: [監視ダッシュボードURL]
- **ログ検索**: [ログ検索URL/クエリ]
- **オンコール**: [PagerDutyリンク/担当チーム]

#### エスカレーション手順

1. **初期対応** (0-15分)
   - [オンコール担当者が実施する手順]
   - [確認すべきダッシュボード]
   - [実行すべきコマンド例]

2. **エスカレーション** (15-30分で解決しない場合)
   - [連絡すべき上位者/チーム]
   - [連絡方法（電話/Slack等）]

3. **インシデント宣言** (30分以上または全停止の場合)
   - [インシデントコマンダーの指名]
   - [全社通知の手順]

#### 外部依存

このサービスが依存している外部サービス:

- [依存サービスA]: [役割]
- [依存サービスB]: [役割]

#### 内部依存

このサービスに依存している内部サービス:

- [内部サービスA]: [影響範囲]
- [内部サービスB]: [影響範囲]

#### 技術スタック

- **言語/フレームワーク**: [例: Python 3.11, Flask]
- **データベース**: [例: PostgreSQL 14]
- **キャッシュ**: [例: Redis 7]
- **インフラ**: [例: Kubernetes on AWS EKS]

#### メトリクスとログ

- **主要メトリクス**:
  - [メトリクス名1]: [説明]
  - [メトリクス名2]: [説明]

- **ログの場所**:
  - [ログシステム名]: [クエリ例]

#### アラート一覧

| アラート名 | 条件 | 対応手順 |
|-----------|------|---------|
| [アラート1] | [閾値条件] | [対応手順の概要] |
| [アラート2] | [閾値条件] | [対応手順の概要] |

---

## まとめ

本ドキュメントで解説した監視運用のベストプラクティス:

1. **アラート**: 緊急性で分類し、適切な通知方法を選択。手順書必須。
2. **オンコール**: 最小4人チーム、1週間ローテ、補償必須。
3. **インシデント管理**: サービス復旧を最優先。ICによるオーケストレーション。
4. **振り返り**: ブレームレス文化。48時間以内に実施。
5. **統計**: Median、パーセンタイル、標準偏差を活用した動的閾値設計。

これらのプラクティスを組み合わせて、持続可能な監視運用体制を構築します。
