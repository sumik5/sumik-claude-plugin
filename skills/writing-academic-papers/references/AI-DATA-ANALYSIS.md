# AIデータ管理・分析ガイド

> データラングリングから探索的分析まで、AI支援によるデータ処理ワークフローの実践

---

## 1. AIデータラングリング・クリーニング

### 核心課題

データラングリングとクリーニングは研究プロセスで最も時間のかかる段階の一つであり、ファイル形式の不統一、欠損値、エラーエントリなど多様な問題に対処する必要がある。小さな見落としでさえ、後続の分析の妥当性を損なう可能性がある。

### AI支援の利点

**自動化・半自動化タスク:**
- 重複排除（deduplication）
- 欠損データ補完（missing data imputation）
- 異常検出（anomaly detection）

**ツール例:**
- Alteryx Designer Cloud（旧Trifacta Wrangler）: 機械学習でデータ変換を推奨
- OpenRefine: 不一致文字列値のマージ提案（例: "U.S.A." vs. "United States"）
- Python: pandas-profiling、great-expectations（データ品質レポート生成）

### ユーザーフィードバックからの学習

AIツールはユーザーの修正を学習し、提案を洗練させる:

1. テキストフィールドの統一（例: "Not applicable"、"N/A"、空白を単一コードに）
2. 確認後、ツールがデータセット全体に自動適用
3. ライブデータ環境で定期更新に対応

### 実践例: 大学データ統合

**シナリオ:** 複数大学から卒業率データを統合

**課題:**
- 大学A: "WD"
- 大学B: "Withdrawn"
- 大学C: "Drop"

**AIソリューション:**
1. 意味的類似性に基づく用語クラスタリング
2. "Withdrawn"に統一する提案
3. 承認後、全関連フィールドで自動統一

**異常検出例:**
- pandas-profilingでfaculty workloadデータの"Hours_Taught"列を分析
- 外れ値「2,000 hours」を検出（データ入力エラーまたは単位ラベルミス）
- モデリング前に早期修正

### 非構造化データのクリーニング

**対象:**
- インタビュー文字起こし
- オープンエンド調査回答

**ツール例（Python NLP）:**
- spaCy
- NLTK

**処理内容:**
- 定型文の削除
- 冗長な免責事項の除去
- 日付スタンプの削除

**ワークフロー例:**
1. PDFドキュメントをAIパイプラインに投入
2. 繰り返し表示される管理ヘッダー・法的免責事項をフィルタリング
3. コアテキストのみ抽出→コーディング・主題分析

### データクリーニングの原則

| 原則 | 内容 |
|------|------|
| **ドキュメント化** | 受け入れた提案、ルールマッピングの詳細ログを保持 |
| **透明性・再現性** | すべてのAI駆動変換を記録 |
| **人間の監視** | AI提案を徹底的に検証 |
| **データ検証** | 変換後のデータ品質を確認 |

### プロンプト例: データクリーニング計画

```
以下のデータセットに対するクリーニング計画を作成してください:

【データセット概要】
- ソース: 5つの大学の学生記録（CSV形式）
- 列: 学生ID、名前、卒業状態、成績、出席率
- 既知の問題: 卒業状態ラベルの不統一、出席率に外れ値

【要求事項】
1. 不統一なラベルの標準化手順
2. 異常値検出基準
3. 欠損値処理方針
4. ステップバイステップのクリーニング手順
```

---

## 2. データ統合・ハーモナイゼーション

### 統合の課題

現代の研究では、データは単一のリポジトリから得られることは稀であり、以下のような異種ソースからデータを統合する必要がある:
- データベース
- API
- アーカイブレポート
- リアルタイムフィード

各ソースは独自の構造とセマンティクスを持つ。

### ノーコード・ローコードプラットフォーム

**利用可能なツール:**
- Zapier
- Make（旧Integromat）
- Airtable

**機能:**
- ドラッグ&ドロップインターフェース
- 異なるサービス接続（Google Sheets、SQLデータベース、Web API）
- AI駆動のフィールドマッピング推奨

### データ統合ベストプラクティス

| フェーズ | アクション |
|---------|----------|
| **計画** | データソース特定、スキーマ理解、統合ゴール定義 |
| **マッピング** | AIが類似列名での結合を提案、日付形式の整合を促す |
| **検証** | 自動提案の確認、バージョン管理、詳細ドキュメント |
| **保守** | 統合後データセットの定期チェック、整合性維持 |

### 実践例1: オンライン調査とCSVの統合

**シナリオ:**
- ソース1: Typeformのオンライン調査（自由記述回答）
- ソース2: 機関人口統計記録（CSV）

**Zapierワークフロー:**
1. 新規調査回答を取り込み
2. CSVでメールアドレスマッチング
3. Airtableの単一テーブルにマージ

**AI機能:**
- Airtableの"Automations"で不一致な人口統計ラベルを統一
- 切り詰められた住所を修正

### 実践例2: 複数Excelファイルの自動統合

**シナリオ:** 複数の共同研究者から頻繁にExcelファイルを受信

**Makeワークフロー:**
1. Google Driveの共有フォルダを監視
2. 受信ファイルを取り込み
3. マスターデータセット（例: Google BigQuery）に行を追加
4. 新しい列や予期しない値タイプをフラグ付け

### 実践例3: テキスト+数値データの統合

**シナリオ:** インタビュー文字起こし + 数値調査データ

**Power Automateワークフロー:**
1. AI駆動テキスト分析（Azure Cognitive Servicesでエンティティ認識）
2. 文字起こしを処理
3. センチメントスコア・トピックラベルを抽出
4. 各参加者の調査回答を含む中央データセットに追加

**利点:** 自由記述コメントと構造化評価の相関分析が容易

### フィールド名のハーモナイゼーション

**課題:**
- 同じ変数に異なるラベル: "SatisfactionScore"、"User_Sat"、"Sat_Score"

**AIソリューション:**
1. ツール（Airtable、OpenRefine）が一貫したフィールド名を提案
2. 推奨を承認
3. プラットフォームが全レコードに変更を適用

### プロンプト例: データ統合計画

```
以下のデータソースを統合する計画を提案してください:

【ソース1】
- 形式: Google Sheets
- 内容: 顧客調査回答（名前、メール、満足度評価）

【ソース2】
- 形式: SQL Database
- 内容: 顧客取引履歴（顧客ID、購入日、金額）

【ソース3】
- 形式: API（JSON）
- 内容: ソーシャルメディアセンチメント（ユーザー名、センチメントスコア）

【要求事項】
1. 統合キーの特定
2. スキーマ整合手順
3. データ品質チェックポイント
4. 統合後の検証方法
```

---

## 3. 探索的データ分析（EDA）とAI

### EDAの目的

正式なモデリング前に、以下を明らかにする:
- 基礎パターン
- 異常
- 関係性

### AI支援EDAの利点

**従来:** 手作業でプロットと要約統計を精査（広大なデータセットでは困難）

**AIツール:**
- 相関分析の自動化
- 重要な相関関係の提案
- 興味深い分布のハイライト

### ノーコードEDAプラットフォーム

**ツール例:**
- Tableau Prep
- DataRobot

**機能:**
- CSV取り込み
- 各変数のヒストグラム自動生成
- 外れ値のハイライト
- 高レベル要約の生成（例: ChatGPT活用）

### 実践例: 学生記録の分析

**データセット:** 全国の異なる学校から10,000件の学生記録

**Tableau Prepワークフロー:**
1. CSVファイル取り込み
2. 各変数のヒストグラム自動生成
3. 出席率の外れ値をハイライト

**ChatGPTによる要約:**
「出席率は最終試験スコアと高い相関があり、特に低所得地区で顕著」

### AIによるタスク自動化

**処理内容:**
- 相関分析
- クラスタリング
- 次元削減

**利点:** コーディングがほとんど不要

### 実践例: 気候データの分析

**シナリオ:** 気候科学者が時系列ファイルをアップロード

**DataRobotワークフロー:**
1. 変数（温度、湿度、汚染レベル）の相関ヒートマップ自動生成
2. 散布図の作成
3. 湿度レベルと特定汚染指標の連動性を発見

### 特徴量エンジニアリング

### 目的

新しい変数を開発してモデル性能と解釈可能性を向上:
- 既存列の組み合わせ
- 生データからの意味信号抽出

### AIツールでの特徴量生成

**Airtable + AI拡張の例:**
- 購入ログから"TimeSinceLastPurchase"列を作成
- "Morning vs. Evening"購入特徴を派生

**ChatGPTの活用:**
- テキスト変換のための正規表現・疑似コード例を生成
- ノーコード環境に直接適応可能

### 自動特徴量エンジニアリング

**ツール例:** H2O Driverless AI

**機能:**
- 変換の提案（数値値のビン化、相互作用変数の作成）

**実践例: 公衆衛生データ:**
- 変数: 日次歩数、BMI、睡眠時間
- AI提案: "過去7日間の平均歩数"、"BMIカテゴリ × 平均睡眠時間"

### テキスト+数値データの統合分析

**シナリオ:** 製品満足度の自由記述回答 + 星評価

**ワークフロー:**
1. MonkeyLearnで各コメントのセンチメントスコア生成
2. Power BIなどのダッシュボードで数値評価とマージ
3. 相関分析→「低星評価でも肯定的センチメント」などの発見

### Wikidataとの統合

**Wikidataの特徴:**
- 大規模な協働維持フリー・オープン知識グラフ
- 数百万のエンティティ（人物、場所、概念など）の構造化リンクデータ

**活用例: ノーベル賞受賞者の分析:**
1. Wikidataから受賞者の標準化メタデータを取得（出生国、所属機関、研究分野）
2. Tableau/Power BIにマージして可視化

**ツール:**
- Wikidata Query Service（SPARQL）
- Python: wikidata2df

**Google Sheetsアドオン:**
- Wikipedia and Wikidata Tools（スプレッドシート内でWikimediaデータベースをクエリ）

### プロンプト例: EDA実行計画

```
以下のデータセットに対する探索的分析計画を作成してください:

【データセット】
- 行数: 5,000件
- 変数: 年齢、性別、収入、購入頻度、製品カテゴリ、満足度スコア

【分析目標】
1. 主要なパターン・相関関係の特定
2. セグメント別の購買行動の違い
3. 満足度に影響する要因の発見

【期待する出力】
- 推奨される可視化手法
- 重点的に調査すべき変数の組み合わせ
- 潜在的な異常値・外れ値の検出方法
```

---

## 4. データ品質ワークフロー

### データ品質チェックリスト

| チェック項目 | 実施内容 |
|------------|----------|
| **完全性** | 欠損値の特定、必須フィールドの確認 |
| **一貫性** | フォーマット統一、単位統一、ラベル標準化 |
| **正確性** | 範囲外の値、論理矛盾、異常値の検出 |
| **適時性** | タイムスタンプ検証、時系列整合性 |
| **一意性** | 重複レコード検出、主キー検証 |

### ステップバイステップ: Google DataPrepワークフロー

**1. セットアップとデータインポート**
- Google Cloud Consoleで"Dataprep"セクションにアクセス
- 新しいDataPrep flowまたはプロジェクトを作成
- Google Cloud Storage（GCS）バケットに接続
- CSV/Excel/JSONファイルをGCSバケットに配置
- "Import Datasets"で選択、プレビュー生成

**2. 初期データ評価**
- データ品質概要の自動生成（欠損値、型ミスマッチ）
- 各列のプロファイル確認（分布、最小/最大値）

**3. テキスト標準化**
- 自由記述列（例: "Customer_Comments"）を選択
- 組み込み"Text Cleanup"機能を使用:
  - 空白のトリミング
  - 大文字/小文字の標準化
  - スペルエラー修正（例: "Smrtphone" → "Smartphone"）
  - 列の分割/マージ

**4. カテゴリフィールドのスマートグループ化**
- 不一致ラベルの処理（例: "Product_Category"）
- "Group Similar Values"を選択
- 提案されたグループ化を承認/調整（例: "Smart Phone"、"smartphone"、"SmarPhone"を統一）

**5. 自動データ品質チェック**
- "Data Health"スキャン実行
- 異常検出（範囲外の評価スコア、無効なメール形式）
- 提案された修正の確認・適用:
  - 評価スコアの標準化
  - 日付形式の修正
  - メールアドレスの修復

**6. データエンリッチメントと検証**
- 派生列の作成（例: 生年月日から"Age Group"）
- 郵便番号の検証（地域固有チェック）
- 時間的整合性の確保（ISO 8601形式への標準化）

**7. 再利用可能レシピの作成とエクスポート**
- すべてのステップを"recipe"として自動ログ
- レシピの命名・バージョン管理
- クリーンなデータセットのプレビュー
- CSV/Excel/データベース/BigQueryへのエクスポート

### 注意事項

| 項目 | 留意点 |
|------|--------|
| **過信の回避** | 自動提案への過度な依存を避ける |
| **バイアス伝播** | 体系的バイアスの防止 |
| **検証** | AI提案の人間による検証 |
| **ドキュメント** | 透明性・再現性のための記録 |
| **倫理的データガバナンス** | データ利用の倫理的配慮 |

---

## 5. まとめ

AIはデータ管理・分析を根本的に変革し、研究者が大規模で複雑な異種データセットを効率的に扱うことを可能にする。データラングリング、クリーニング、統合、探索的分析などの退屈だが重要なタスクを自動化することで、AI駆動ツールは研究生産性、正確性、再現性を大幅に向上させる。

**AIの利点:**
- 隠れたパターンの迅速な発見
- エラーの修正
- 異種ソースのハーモナイゼーション
- 洞察に満ちた新しい特徴量のエンジニアリング

**人間の役割:**
- 厳格な検証
- 倫理的配慮
- 透明なドキュメント化
- 自動提案の検証
- 体系的バイアスの防止
- 科学的結論の妥当性確保

AIは学術的専門知識を置き換えるのではなく、大幅に増強する。研究者を単調なタスクから解放し、データとのより深い関わり、革新的発見の促進、学術研究の全体的な厳密性の向上を実現する。
