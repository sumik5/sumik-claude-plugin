# GREEN-AI.md

グリーンAI/ML（機械学習・大規模言語モデル）エンジニアリング。

---

## 1. AI/MLの環境インパクト

### 成長率の爆発

**Moore's lawを超える速度:**

- 1950-2012年: AIモデルサイズはMoore's lawに従って成長
- 2012年以降: モデルサイズが爆発的に増加
- 2023-2024年: 数千億パラメータのモデルが一般化（Transformer（2.13億パラメータ）の約1,000倍）

### カーボンコストの増大

**Transformerモデルの例（2019年論文）:**

- 1回のニューラルアーキテクチャサーチ学習: アメリカの自動車5台分の生涯排出量に相当
- 当時は「レア」と批判されたが、現在は大規模モデルが標準化

### 学習（Training）と推論（Inference）

- **学習**: モデルを1回または数回実行
- **推論**: 何度も何度も実行（本番運用）
- エンタープライズソフトウェアでは推論フェーズのカーボンコストが最大になる可能性

### AI利用の急成長

**McKinsey "State of AI in 2022":**
- 2017-2022年: AI採用率が2倍以上に増加
- 2018-2022年: 組織が使用するAI機能数も2倍に増加
- NLP（自然言語処理）がリード

**Deloitte "State of AI in the Enterprise" 2022:**
- 94%の回答者: AIが成功に不可欠
- 79%の回答者: 3種類以上のAIを完全展開（2021年は62%）
- 2023年初頭のLLMブレイクスルーでさらに加速が予想

---

## 2. MLライフサイクル概要

```
プロジェクト計画
    ↓
データ収集
    ↓
モデル設計・学習
    ↓
デプロイ・メンテナンス
```

各フェーズでグリーン化の機会が存在。

---

## 3. プロジェクト計画

### 設計段階での問い

- **気候インパクトはどうなるか？**
- **どう測定するか？**
- 紙の製品段階での変更は、実装後よりはるかに安価

### カーボンアウェア機能の組み込み

**デマンドシェイピング（Demand Shaping）の検討:**

- グリッドのカーボンインテンシティに応じた動作変更
- 例: ビデオ会議の画質調整（帯域幅に応じる）と同様の発想
- グリッドが「dirty」な時は計算量の少ないレコメンデーションを提供

### SLA/SLOの見直し（重要）

**Adrian Cockcroft（元AWS VP Sustainable Architecture）の指摘:**

> "The biggest win is often changing requirements or SLAs. Reduce retention time for log files. Relax overspecified goals."

**具体的なアクション:**
- ログファイル保持期間短縮
- 過剰指定された目標の緩和
- 実際に必要なサービスレベルターゲットの批判的検討
- 必要以上を提供しない

---

## 4. データ収集

### データ収集の実態

- MLライフサイクルで「地味」な作業と見なされがち
- 用語: "crap in, crap out", "data cleaning", "panning for gold"
- しかし、データ品質問題は精度低下等の影響が大きい

### 大規模データセットの必要性

- **オーバーフィッティング（Overfitting）防止**
- **実世界データの代表性確保**
- **再利用可能性**（他プロジェクトで活用）

### グリーンデータ収集戦略

#### 1. 既存データセット活用（最優先）

**オープンソースデータセット:**

| プラットフォーム | データセット数 | 特徴 |
|-----------------|--------------|------|
| Hugging Face | 110,000以上 | オープンソース、無料 |
| Kaggle | 300,000以上 | オープンソース、無料 |

- 例: 猫画像、アボカド価格、SAARC諸国の新生児死亡率等
- 既存データ使用 = 新規パイプラインのカーボンエミッション不要

#### 2. データ量の批判的評価

- 実際に必要なデータ量を精査
- 過剰なデータ収集を避ける

#### 3. デマンドシフティング

- オンデマンドでない場合、グリーンエネルギー利用可能時に実行
- 詳細はCh5（Carbon Awareness）参照

### 倫理的配慮（重要）

**インフォームドコンセント:**
- データセット構築・使用における同意の欠如
- 後から同意撤回が不可能な場合がある
- AI生成画像（アート業界）での議論
- コード自体がAIモデルに使用される可能性

**RLHF（Reinforcement Learning with Human Feedback）の問題:**
- 生データ（Raw data）: ラベルなし、インターネットから大量スクレイピング
- 暴力的・性差別的・人種差別的コンテンツの存在
- RLHFによる対策: 生データ + 人間ラベル付きデータ
- 問題: ケニア労働者が時給2ドル未満で有害コンテンツをラベリング（TIME 2023年1月報道）

---

## 5. モデル設計・学習

### サイズの削減（Size Matters）

**大規模モデルの課題:**
- 大量のストレージと計算サイクルが必要
- 学習時間が長い

**サイズ削減のメリット:**
- 学習時間短縮
- リソース効率向上
- 時間・コスト・カーボン削減
- エッジコンピューティング・IoTデバイス対応

**注意**: リソース効率化だけでは不十分。同じハードウェア・エネルギーでより多くを達成できる可能性を解放する。

#### モデルサイズ削減技法

**1. 量子化（Quantization）**

- 定義: 連続無限値を有限離散値にマッピング
- MLでは: 32ビット浮動小数点→8ビット整数等
- メリット:
  - ストレージ削減
  - 整数演算が高速→エネルギー削減
- 量子化アウェア学習（Quantization-Aware Training, QAT）:
  - 学習フェーズで実行
  - Meta LLaMAモデル例: 7B/13B/30Bで4ビット量子化に成功
  - LLM初の成功例、リソース効率的な学習への扉を開く

**2. プルーニング（Pruning）**

- 定義: 不要なニューロンを削除
- "PruneTrain: Fast Neural Network Training by Dynamic Sparse Model Reconfiguration" 論文:
  - 画像分類シナリオで学習時間・FLOPs・メモリ・アクセラレータ間通信を大幅削減

**3. 蒸留（Distillation）**

- 定義: 大規模モデルの知識を小規模モデルに転移
- より小さく効率的なモデルを作成

**4. 圧縮（Compression）**

- 一般的なモデル圧縮技術

#### 副次的メリット: ML研究の民主化

- 小規模モデル: ラップトップで研究可能
- 大規模モデル: 資金潤沢な機関のみ
- 公平性問題の緩和

### エネルギー効率の良いハードウェア選択

**専用AIチップ（Specialized AI Chips）:**

| チップタイプ | 用途 |
|------------|------|
| GPU（Graphics Processing Units） | 学習・推論 |
| FPGA（Field-Programmable Gate Arrays） | 学習・推論 |
| ASIC（Application-Specific Integrated Circuits） | 学習・推論 |

**効率性:**
- 専用チップ vs CPU: **10-1,000倍の効率向上**
- 画像認識等で歴史的に活用
- コスト削減効果も大きい

**エンボディドカーボン考慮:**
- ハードウェア製造時のカーボンコストも評価必要（Ch6参照）

### カーボンアウェアな学習スケジューリング

**MLトレーニングの特徴:**
- 緊急性が低い場合が多い
- デマンドシフティングに最適（Ch5参照）
- グリーンエネルギー利用可能時に実行

### Transfer Learning / Fine-Tuning活用

**1. 既存モデルをそのまま使用**

- 学習フェーズをほぼスキップ→最もグリーン
- Hugging Face: 400,000以上のモデル
- Kaggle: 2,000以上のモデル
- 公開またはコスト購入可能

**2. Transfer Learning（転移学習）**

- 定義: 大規模データセットで学習した汎用モデルを新目的に再利用
- 例: 猫検出モデル→猿検出モデルに適応
- 技法:
  - **Fine-tuning（微調整）**: 既存モデルのパラメータを少し調整
  - **Feature extraction（特徴抽出）**: 既存モデルの一部を凍結し、新レイヤーのみ学習
- メリット:
  - ゼロから学習するよりカーボン削減
  - データ収集も不要な場合がある
  - 経済的にも有利（CFOへのアピールポイント）

### エポック数・ハイパーパラメータ最適化

- 必要以上のエポック数を避ける
- ハイパーパラメータ探索を効率化

### エッジ学習（Edge Training）

- データセンター集約学習の代替
- デバイス近くで学習実行
- グリーン化の可能性

### フェデレーテッドラーニング（Federated Learning, FL）

**概要:**
- Google 2017年から本番運用
- エンドユーザーデバイスで分散学習
- 全データはデバイスにローカル保持
- 最終モデルのみ協調

**"Can Federated Learning Save the Planet?" 論文:**
- 集約学習 vs FL（カーボンコスト比較）
- 収束は遅いが、小規模データセット・低複雑性モデルではFL がグリーン

---

## 6. デプロイ・メンテナンス

### 推論フェーズの重要性

**本番企業での現実:**
- 学習: 1回または数回
- 推論: 何千回、何万回、何百万回
- **推論フェーズが最大のカーボン消費源の可能性**
- 研究論文執筆が目的の場合は異なるが、エンタープライズソフトウェアでは推論に注目

### モデルサイズ削減（本番運用）

**ポストトレーニング技法:**
- 量子化
- 圧縮
- プルーニング

**メリット:**
1. **小型デバイス対応**: IoT・クライアントサイドシナリオ
2. **リソース効率向上**: 小型モデル = デプロイが安価かつグリーン

### 推論の最適化

**バッチ処理:**
- 複数リクエストをまとめて処理
- スループット向上
- GPU/TPU利用率向上

**キャッシング:**
- 頻繁なクエリ結果をキャッシュ
- 再計算削減

### エッジコンピューティングでの推論

- データソース近くで処理・ストレージ
- エネルギー消費削減（ネットワーク伝送削減）
- レイテンシ改善

### MLOps（Machine Learning Operations）

**定義:**
- ML × DevOps × Data Engineering の交差領域
- ML モデルの本番運用・保守・監視を体系化

**グリーン化の共通点:**
- DevOps の教訓多数適用可能（Ch4 Operational Efficiency参照）
- コード効率（Ch3 Code Efficiency）も適用可能

### モデルの定期的見直し・更新

- 不要なモデルの削除
- 非効率なモデルの置き換え
- 最新の効率化技法の適用

---

## 7. どこから始めるべきか？

### プロジェクト特性別の優先順位

**大規模モデル + 新規データ:**
- **優先**: データ収集の最適化

**超高精度要求 / 研究シナリオ:**
- **優先**: 学習コストの最適化
- 本番運用しない場合

**本番運用中のML:**
- **優先**: デプロイ・メンテナンスフェーズ
- 推論が最大のカーボン消費源の可能性

### 測定の重要性

- フェーズごとのカーボンフットプリント測定
- プロジェクト固有の特性を反映

---

## 8. グリーンAI実践チェックリスト

### プロジェクト計画

- [ ] 気候インパクトの事前評価
- [ ] 測定方法の決定
- [ ] カーボンアウェア機能の設計
- [ ] SLA/SLO の批判的見直し

### データ収集

- [ ] 既存データセット（Hugging Face/Kaggle等）の調査
- [ ] 必要データ量の精査
- [ ] デマンドシフティングの適用検討
- [ ] 倫理的配慮の確認

### モデル設計・学習

- [ ] モデルサイズ削減技法の適用（量子化・プルーニング・蒸留）
- [ ] 既存モデル再利用の検討
- [ ] Transfer Learning / Fine-Tuning の活用
- [ ] 専用AIチップの利用
- [ ] カーボンアウェアなスケジューリング
- [ ] エッジ学習・フェデレーテッドラーニングの検討

### デプロイ・メンテナンス

- [ ] ポストトレーニングでのモデル最適化
- [ ] 推論のバッチ処理・キャッシング
- [ ] エッジコンピューティングでの推論
- [ ] MLOps ベストプラクティスの適用
- [ ] 定期的なモデル見直し

---

## 9. 参考リソース

### データセット

- **Hugging Face**: 110,000以上のデータセット
- **Kaggle**: 300,000以上のデータセット

### モデル

- **Hugging Face**: 400,000以上のモデル
- **Kaggle**: 2,000以上のモデル

### 学習リソース

- TensorFlow: Transfer Learning and Fine-Tuning ガイド
- 各種論文:
  - "Energy and Policy Considerations for Deep Learning in NLP" (Strubell et al., 2019)
  - "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models" (Meta)
  - "PruneTrain: Fast Neural Network Training by Dynamic Sparse Model Reconfiguration"
  - "Can Federated Learning Save the Planet?"
