# テストと評価ガイド

> スキルの品質を確保するための構造化テスト手法。基本的な評価駆動開発（Evaluation-Driven Development）やTwo-Claude Patternは`WORKFLOWS.md`を参照。

## 目次

- [1. ユースケース分類](#1-ユースケース分類)
- [2. 成功メトリクス](#2-成功メトリクス)
- [3. 3領域テストフレームワーク](#3-3領域テストフレームワーク)
- [4. イテレーション戦略](#4-イテレーション戦略)

---

## 1. ユースケース分類

Anthropicの観察によると、スキルのユースケースは3つのカテゴリに分類される。各カテゴリは異なるテスト戦略を必要とする。

### Category 1: ドキュメント・アセット生成（Document & Asset Creation）

**用途:** ドキュメント、プレゼンテーション、アプリ、デザイン、コードなど、一貫性と品質の高い成果物を生成する。

**実例:** `frontend-design` スキル（他に `docx`, `pptx`, `xlsx`, `pdf` スキル等）

```
高いデザイン品質で独自性のあるプロダクショングレードのフロントエンドインターフェースを作成。
Webコンポーネント、ページ、artifacts、ポスター、アプリケーションの構築時に使用。
```

**キーテクニック:**

- **埋め込みスタイルガイド**: ブランド基準やデザインルールをスキルに組み込む
- **テンプレート構造**: 一貫した出力を保証するテンプレート定義
- **品質チェックリスト**: 最終化前の検証項目
- **外部ツール不要**: Claudeの組み込み機能（Code Execution等）のみで完結

**テストポイント:**

- 生成物が仕様・スタイルガイドに準拠しているか
- ブランド一貫性が保たれているか
- 初回生成の品質（ユーザー補正なしで使用可能か）

---

### Category 2: ワークフロー自動化（Workflow Automation）

**用途:** 一貫した方法論で実行される多段階プロセス。複数のMCPサーバー連携を含む。

**実例:** `skill-creator` スキル

```
新しいスキルを作成するための対話的ガイド。ユースケース定義、frontmatter生成、
指示記述、バリデーションまでユーザーをウォークスルー。
```

**キーテクニック:**

- **ステップバイステップワークフロー**: 検証ゲートを伴う段階的実行
- **共通構造のテンプレート**: 繰り返し使えるパターン化
- **レビューと改善提案**: ビルトインのフィードバックループ
- **反復改善ループ**: 品質基準に達するまで繰り返す

**テストポイント:**

- ワークフロー全体が途中で停止せず完了するか
- 各ステップの検証が正しく機能するか
- エラー発生時のロールバック・リトライが動作するか

---

### Category 3: MCP連携強化（MCP Enhancement）

**用途:** MCPサーバーが提供するツールアクセスにワークフローガイダンスを追加。

**実例:** Sentryの `sentry-code-review` スキル

```
SentryのエラーモニタリングデータをMCPサーバー経由で取得し、GitHub Pull Requestで検出されたバグを
自動分析・修正する。
```

**キーテクニック:**

- **複数MCPコールの順次調整**: 複数ツールを適切な順序で呼び出す
- **ドメイン専門知識の埋め込み**: ユーザーが本来指定すべき知識をスキルに組み込む
- **コンテキスト提供**: ユーザーが毎回説明する必要のない情報を自動補完
- **MCPエラーハンドリング**: API失敗の一般的ケースへの対処

**テストポイント:**

- MCP接続が正しく確立されるか
- ツールコールが適切な順序・パラメータで実行されるか
- API失敗時のリトライ・エラーメッセージが適切か

---

## 2. 成功メトリクス

スキルの効果を評価するための定量・定性メトリクス。これらは**目標値**であり、厳密な閾値ではない。

### 定量的メトリクス

| メトリクス | 目標値 | 測定方法 |
|-----------|--------|----------|
| **トリガー率** | 90% | 関連クエリ10-20個を実行し、自動トリガーされた割合を追跡 |
| **ツールコール数** | ベースラインより減少 | スキルあり/なしで同一タスクを実行し、ツールコール数と総トークン数を比較 |
| **失敗API数** | 0 | MCPサーバーログを監視し、リトライ率とエラーコードを追跡 |

### 定性的メトリクス

| メトリクス | 評価基準 | 評価方法 |
|-----------|---------|----------|
| **ユーザー補正不要** | 次ステップの指示が不要 | テスト中にリダイレクトや補足説明が何回必要か記録。ベータユーザーからのフィードバック |
| **ワークフロー一貫性** | 補正なしで完了 | 同一リクエストを3-5回実行し、出力の構造的一貫性と品質を比較 |
| **初回成功率** | 新規ユーザーが初回で成功 | 新規ユーザーが最小限のガイダンスで初回にタスクを完了できるか |

### メトリクス設計テーブル例

| ユースケース | 定量メトリクス | 定性メトリクス | 目標 |
|------------|-------------|-------------|------|
| プロジェクト計画 | トリガー率90%、ツールコール数6以下、失敗API 0 | 補正不要、一貫性、初回成功 | 15往復→2往復に短縮 |
| デザイン生成 | トリガー率85%、トークン消費50%削減 | スタイルガイド遵守、ブランド一貫性 | ユーザー修正0回 |
| コンプライアンスチェック | API失敗0、監査ログ100%記録 | 規制準拠、透明性 | 承認プロセス自動化 |

---

## 3. 3領域テストフレームワーク

スキルの品質を保証するための3つのテスト領域。

### 3.1 トリガーテスト

**目標:** 適切な場面で発火し、不適切な場面では発火しない。

#### ✅ Should trigger（発火すべき）

スキルが自動的にロードされるべきクエリ例:

```
【ProjectHub スキルの例】
- "Help me set up a new ProjectHub workspace"
- "I need to create a project in ProjectHub"
- "Initialize a ProjectHub project for Q4 planning"
- "ProjectHub でプロジェクト作成して"
```

#### ❌ Should NOT trigger（発火すべきでない）

スキルが反応すべきでないクエリ例:

```
【ProjectHub スキルの例】
- "What's the weather in San Francisco?"
- "Help me write Python code"
- "Create a spreadsheet"（ProjectHub がスプレッドシートを扱わない場合）
- "Show me Linear projects"（別サービス）
```

#### テスト実施方法

1. **トリガー候補クエリを10-20個作成**（明示的 + パラフレーズ + 否定例）
2. **クエリを実行**
3. **スキルがロードされたか確認**:
   - **Claude Code (CLI)**: `/context` コマンドを実行し、アクティブなスキル一覧でスキルがロードされているか確認
   - **Claude.ai**: 右上の「Skills」ドロップダウンでアクティブなスキルを確認
4. **トリガー率を計算**（目標: 関連クエリの90%）

#### Claude Code でのスキル確認方法

Claude Code（CLI）では `/context` コマンドでロード済みスキルを確認できる:

```
> /context
# 出力例:
# Active Skills:
#   - writing-clean-code (auto)
#   - developing-nextjs (auto)
#   - my-new-skill (manual)
```

スキルのトリガーテスト時は、以下の手順で確認:

1. 新しいセッションを開始
2. トリガー対象のクエリを入力
3. `/context` でスキルがロードされたか確認
4. ロードされていない場合、`description` のトリガー条件を調整

---

### 3.2 機能テスト

**目標:** スキルが正しい出力を生成する。

#### テストケース例（Given/When/Then形式）

```
【ProjectHub プロジェクト作成のテスト】

Test: 5つのタスクを持つプロジェクト作成

Given:
  - プロジェクト名 "Q4 Planning"
  - 5つのタスク説明

When:
  - スキルがワークフローを実行

Then:
  ✅ ProjectHub にプロジェクトが作成されている
  ✅ 5つのタスクが正しいプロパティで作成されている
  ✅ 全タスクがプロジェクトにリンクされている
  ✅ API エラーが発生していない
```

#### 機能テストチェックリスト

- [ ] **有効な出力が生成される**（フォーマット、完全性、精度）
- [ ] **API呼び出しが成功する**（MCPツールコールがエラーなく完了）
- [ ] **エラーハンドリングが機能する**（API失敗時のリトライ・フォールバック）
- [ ] **エッジケースがカバーされている**（空入力、大量データ、特殊文字等）

---

### 3.3 性能比較テスト

**目標:** スキルなしと比較して改善を証明する。

#### ベースライン比較テーブル例

| メトリクス | スキルなし | スキルあり | 改善率 |
|-----------|----------|----------|--------|
| 往復メッセージ数 | 15回 | 2回 | **-87%** |
| 失敗APIコール数 | 3回 | 0回 | **-100%** |
| トークン消費量 | 12,000 | 6,000 | **-50%** |
| ユーザー補正回数 | 5回 | 0回 | **-100%** |
| タスク完了時間 | 10分 | 2分 | **-80%** |

#### 性能比較の実施方法

1. **同一タスクを2つのセッションで実行**（スキルON/OFF）
2. **上記メトリクスを記録**
3. **改善率を計算**
4. **ユーザーに見せるための比較表を作成**

---

## 4. イテレーション戦略

### Pro Tip: 単一タスクで反復してから拡張

最も効果的なスキル開発者は、**1つの難しいタスクでClaudeが成功するまで反復し、その成功アプローチをスキルに抽出する**。これはClaude のin-context learning を活用し、幅広いテストよりも速いフィードバックを提供する。

**ワークフロー:**

1. **難しいタスクを1つ選ぶ**（典型的かつ複雑なもの）
2. **Claudeと対話しながら成功させる**（プロンプトを調整・補足情報を追加）
3. **成功したアプローチをスキルに抽出**（プロンプト→SKILL.md）
4. **他のテストケースに拡張**

---

### Undertriggering の兆候と対策

**兆候:**

- スキルが本来ロードされるべき場面でロードされない
- ユーザーが手動でスキルを有効化している
- 「このスキルをいつ使うか」についてのサポート質問が多い

**対策:**

1. **descriptionフィールドに詳細とニュアンスを追加**

```yaml
# Before（曖昧すぎる）
description: Helps with projects.

# After（具体的なトリガーフレーズ追加）
description: Analyzes Figma design files and generates developer handoff documentation.
Use when user uploads .fig files, asks for "design specs", "component documentation",
or "design-to-code handoff".
```

2. **技術用語・キーワードを追加**（ユーザーが実際に言う言葉）

3. **トリガークエリのバリエーションをdescriptionに含める**

---

### Overtriggering の兆候と対策

**兆候:**

- スキルが無関係なクエリでロードされる
- ユーザーがスキルを無効化している
- スキルの目的について混乱が生じている

**対策:**

1. **ネガティブトリガーを追加**（「使用しない場合」を明記）

```yaml
description: Advanced data analysis for CSV files. Use for statistical modeling,
regression, clustering. Do NOT use for simple data exploration (use data-viz skill instead).
```

2. **スコープを限定**

```yaml
# Before（広すぎる）
description: Processes documents.

# After（具体的に限定）
description: Processes PDF legal documents for contract review. Use specifically for
legal contract workflows, not for general document processing.
```

3. **用途を明確化**

```yaml
description: PayFlow payment processing for e-commerce. Use specifically for online
payment workflows, not for general financial queries.
```

---

### 指示実行問題の対策

**症状:** スキルはロードされるが、Claudeが指示に従わない。

**原因と対策:**

| 原因 | 症状 | 対策 |
|-----|------|------|
| **指示が冗長すぎる** | 重要な指示が見逃される | - 簡潔に書く<br>- 箇条書き・番号リスト使用<br>- 詳細は別ファイルに移動 |
| **重要指示が埋もれている** | クリティカルなステップがスキップされる | - 重要な指示を冒頭に配置<br>- `## Important` や `## CRITICAL` ヘッダー使用<br>- 必要なら繰り返す |
| **曖昧な言語** | 解釈にばらつきが生じる | - 具体的・アクション可能に記述<br>- 検証スクリプトを添付<br>- コード例を示す |

**例: 曖昧 vs 具体的**

```markdown
# ❌ Bad（曖昧）
Make sure to validate things properly.

# ✅ Good（具体的）
CRITICAL: Before calling create_project, verify:
- Project name is non-empty
- At least one team member assigned
- Start date is not in the past
```

**高度なテクニック:**

クリティカルな検証では、言語指示ではなく**検証スクリプトをバンドル**する方が確実。コードは決定論的、言語解釈は不確実。`Office` スキル群がこのパターンの好例。

---

### skill-creator の活用

`skill-creator` スキルはテストとイテレーションを支援する:

**スキル作成:**

- 自然言語記述からスキル生成
- 適切なフォーマットのSKILL.mdとfrontmatter生成
- トリガーフレーズと構造を提案

**スキルレビュー:**

- 一般的な問題をフラグ（曖昧なdescription、トリガー不足、構造的問題）
- over/under-triggering のリスクを特定
- スキルの目的に基づくテストケースを提案

**反復改善:**

エッジケースや失敗例に遭遇したら、それを `skill-creator` にフィードバック:

```
「このチャットで特定された問題と解決策を使って、スキルが[特定のエッジケース]を
処理する方法を改善してください。」
```

**使用方法:**

```
"Use the skill-creator skill to help me build a skill for [your use case]"
```

**注意:** `skill-creator` はスキルの設計と改善を支援するが、自動テストスイートの実行や定量評価結果の生成は行わない。

---

## まとめ

スキルの品質確保には、以下の3つのテストを組み合わせる:

1. **トリガーテスト** - 適切な場面で発火するか
2. **機能テスト** - 正しい出力を生成するか
3. **性能比較テスト** - ベースラインより改善しているか

イテレーション時は、**単一タスクで成功させてから拡張**し、undertriggeringとovertriggeringの兆候を監視しながら調整する。

詳細なワークフローと評価駆動開発パターンは `WORKFLOWS.md` を参照。
